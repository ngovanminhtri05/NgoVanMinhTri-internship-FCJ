[{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"University of California Irvine Backs Up Petabytes of Research Data to AWS","tags":[],"description":"","content":"University of California Irvine Backs Up Petabytes of Research Data to AWS By Philip Papadopoulos, Abhijeet Lokhande, Evan Wood, Francisco Ramon Lopez, and Nicholas Santucci | 29 MAY 2025\nCategories: Amazon Athena, Amazon CloudWatch, Amazon DynamoDB, Amazon EventBridge, Amazon S3 Glacier Deep Archive, Amazon Simple Notification Service (SNS), Amazon Simple Storage Service (S3), AWS Lambda\nEditor\u0026rsquo;s Note AWS is not responsible for UCI\u0026rsquo;s public GitHub repo linked in this post, which has been provided so that interested parties can explore the solution described in this post in more detail.\nThe University of California, Irvine (UCI) is a public land-grant research university with troves of research data stored on servers in lab environments on about 1500 faculty-research lab environments on campus. UCI needed a solution to address the practical and economic challenge of providing centralized backups for these independently-administered servers.\nThe goal for the UCI Research Cyber Infrastructure Center (RCIC) is to provide the tooling and backup storage capacity server owners need to perform backups while also providing more data protection in the event of deletions, accidental or otherwise. The UCI RCIC surveyed the existing backup solutions deployed independently around campus and found none that meet their cost requirements, dynamic range of scale, and accommodations for the reality of central as opposed to distributed system administration. Existing outliers of systems can have as much as a Petabyte on single server and upwards of one billion files. UCI had about 100 servers with an estimated total storage of 10 PB that needed to be backed up.\nIn this post we walk through the design choices for UCI\u0026rsquo;s centralized backup solution, including how Amazon S3 buckets, AWS Identity and Access management (IAM) roles, and AWS accounts are programmatically created per-server to provide the proper isolation. We will also provide an overview of how specific requirements are mapped to AWS services and rclone invocation. With this solution, UCI was able to backup over 5 PB of data (over 250 million files) with an efficient, scalable system that featured a compact and easily maintainable amount of code.\nSolution Overview The UCI RCIC developed a scalable backup solution to address the diverse needs of their 1500 faculty research labs. The implementation framework centers on a custom-developed system that uses rclone for data movement while incorporating multiple AWS services for security and data management. This solution was specifically designed to handle the wide variance in server sizes across campus, ranging from 1 TB to 2 PB, with a total storage requirement of approximately 10 PB across 100 servers.\nThe implementation architecture establishes clear separation between system administration and cloud administration roles, enhancing security through segregation of duties. On-premises servers (Lab1 and LabN) use rclone to perform daily backups to S3 buckets. The S3 buckets are configured with S3 Lifecycle policies that move data to S3 Glacier Deep Archive for long-term storage. The system is monitored using Amazon CloudWatch with custom dashboards and alarms, while notifications are handled through Amazon Simple Notification Service (Amazon SNS). Additional AWS services and features like Amazon DynamoDB, AWS Lambda, AWS Step Functions, S3 Batch Operations, and Amazon EventBridge are used to control access and describe policy.\nArchitecture Diagrams Figure 1: High-level view of backup to Amazon S3 using rclone. A myriad of AWS services are used to control access and describe policy. A custom Python3 wrapper around the open source tool rclone streamlines the description of backup jobs.\nKey Components To maintain cost efficiency while ensuring data protection, UCI RCIC developed a custom Python wrapper that streamlines backup job definitions and execution. This wrapper interfaces with AWS services including S3, IAM, Amazon CloudWatch, and Amazon SNS to provide comprehensive monitoring and management capabilities.\nFigure 2: A custom Python wrapper around the open source tool rclone streamlines the description of backup jobs\nThe solution incorporates three primary operational components:\nDaily incremental backups for efficient data protection Weekly deep syncs for comprehensive verification Automated lifecycle management for cost optimization Implementing this tiered approach allows UCI to successfully balance the competing demands of performance, cost, and data security while maintaining system availability for research operations.\nFeature Implementation Matrix Feature/Capability Technology/Service Control Data movement/backup rclone sysadmin Backup job definition Custom yaml file sysadmin Backup job execution UCI-Developed code wrapping rclone sysadmin (in GitHub) Backup job scheduling Unix Cron, Windows Scheduled tasks sysadmin Deletion of files on backup rclone running in sync mode sysadmin File overwrite/deletion protection S3 Versioning cloudadmin Permanent deletion of backup S3 Lifecycle policy cloudadmin Early deletion of data S3 Admin – MFA required cloudadmin Backup destination provision Custom scripts that target S3 and apply policy cloudadmin IP network limitations IAM access policy cloudadmin Overview and per-server view Cloudwatch dashboards cloudadmin Quotas and activity alarms Cloudwatch alarms cloudadmin Restore files S3 Glacier restore + custom python cloudadmin/sysadmin Approach to Centralized Backups A. Core Data Movement Why rclone? Rclone offers support for both Amazon S3 storage and local file systems, with built-in handling of S3 object versioning and direct integration with cloud storage APIs. The tool employs an incremental file system exploration methodology, enabling memory-efficient processing of large file systems, which has been verified with systems containing over 50 million files. When rclone needs to synchronize the contents of the local file system with the copy in Amazon S3, it explores the local file system incrementally. UCI has single backup jobs with over 50 million files and rclone fully explores the file system without exhausting system memory.\nRclone is also natively multi-threaded. For example, when making the first \u0026ldquo;full\u0026rdquo; backup of a 1 PB file server, UCI had \u0026ldquo;scaled back\u0026rdquo; rclone so that it did not saturate the 10 GbE link and the file server was still capable of serving files. In short, it\u0026rsquo;s performant but can be governed so that it doesn\u0026rsquo;t make the system unavailable.\nUCI developed a small Python script (gen-backup.py in UCI\u0026rsquo;s public GitHub repository) that wraps Rclone. This code interprets a YAML-formatted file that defines one or more backup jobs.\nSample Backup Job Definition ## Path is common to the jobs relative to the path --- srcpaths: - path: /datadir ## Local decision to exclude .git subdirs exclude_global: - .git/** ## Patterns from a file to exclude exclude_file: common_excludes.yaml jobs: - name: backup1 subdirectories: - DataImages In a nutshell, the job called \u0026ldquo;backup1\u0026rdquo; replicates all files in the path /datadir/DataImages and excludes common file patterns. To run all backup jobs, only the following command needs to be issued:\ngen-backup.py run There are numerous options to gen-backup to govern parallelism, which jobs are run, and in what mode they are run.\nData Transfer Implementation Steps Configuration: UCI configured individual backup profiles for each research lab server Job definition: Created standardized YAML templates for consistent backup definitions Execution: Implemented automated backup processes through custom wrapper Monitoring: Established lab-specific monitoring protocols Optimization: Developed transfer rate controls based on server capacity Scheduling: Created lab-specific backup schedules based on data change rates This structured approach ensures consistent, reproducible backup operations while maintaining system performance and reliability within defined operational parameters.\nB. Storage Quota for Labs The central campus IT team manages backup costs, necessitating implementation of specific controls to prevent cost overruns. To achieve this, the following factors needed to be understood for each server:\nHow much data in terms of volume (in terabytes) is stored? How many files are in the system (in 1 million object increments)? Does access to the backup S3 bucket look \u0026ldquo;abusive\u0026rdquo; (too many transactions)? Has a system made no API calls to the bucket (backup could be off on the server)? Each one of these bullets is configured as a CloudWatch alarm. Both the cloudadmin and the appropriate sysadmin(s) are notified whenever any of these alarms are triggered using Amazon SNS. There is a dashboard that allows cloudadmins to see the state of these alarms for all servers that are being backed up.\nFigure 4: Alarms set up per storage server: storage and object quotas and activity monitoring\nOne such alarm monitored API calls, which was expressed as a ratio of requests to objects. If this ratio exceeds 3.5, then the alarm is triggered. 3.5 was chosen as an engineering cushion to not alarm under most operating circumstances.\nOver the almost year-long period of experience, these alarms have proven useful to alert UCI on various states—especially on terabytes and number of objects limits.\nC. Monitoring and Visibility The monitoring system needed to accommodate 50-100 servers operating on independent backup schedules. With end systems being managed by disparate and disjoint administrators, only the data in AWS can be used as a central aggregation point. For cost containment, UCI is most concerned with averages. Some servers may have many small files or significant data churn (resulting in higher percentages of non-current objects) and thus are \u0026ldquo;more expensive per terabyte.\u0026rdquo; These are balanced by other servers that have relatively large files and/or low data churn and thus are \u0026ldquo;less expensive per terabyte.\u0026rdquo;\nUCI built a custom dashboard in CloudWatch that combines metrics from all backup buckets, teases out different API calls to estimate ongoing cost. The top widget shows 2780 TB total storage across 186 million files. Of this total storage 2690 TB is in S3 Glacier Deep Archive, and 454 TB is held as deleted or overwritten files that have not been permanently deleted through an S3 Lifecycle policy. The last figure indicates a 16.3% overhead for retaining deleted/changed objects.\nFigure 5: Overall storage, API, and cost utilization across all servers\nThe weekly spikes on the API chart are from a weekly deep sync. A careful inspection also shows a ramp up of standard storage 2 TB at beginning of the monitoring window and 88.1 TB at the end. This is explained by the onboarding of a new 1 PB server.\nThe dominant fraction of UCI\u0026rsquo;s monthly backup bill is owed to the cost of storage itself, which accounts for about 80% of monthly cost. An analysis of the July 2024 bill showed:\n80% storage 1.5% CloudWatch 18.5% API An additional cost not shown that is important to be cognizant of is the S3 Lifecycle transition cost when data is moved from Amazon S3 Standard (where it is initially placed) to S3 Glacier Deep Archive.\nPer-Server Monitoring The overview dashboard is helpful, but per-server high-level views are also critical to insight. A second cost estimate dashboard was configured to get this detailed insight. Systems at UCI range in capacity from 11 TB to 1040 TB and object counts in the range of 652,000 to 59.4 million. This dashboard replicates the top line information of the roll-up view, but it is tailored to each system.\nFigure 6: Per-server storage utilization and cost\nAll dashboards shown are built programmatically so that they are easily replicated in another environment.\nD. Performant Backups When dealing with large data servers, volume (terabytes) and number of files (number of objects) are critical factors that affect performance. It\u0026rsquo;s perhaps easiest to illustrate these orders of magnitude with some examples:\n1024 TB @ 1 gbps – 10 million seconds to transmit (100 Days) 1024 TB @ 10 gbps – 1 million seconds to transmit (10 days) 50 million file syncs at 100 checks/second = 500,000 seconds (5.8 days) 50 million file syncs at 1000 checks/second = 50,000 seconds (13.8 hours) For backup to the cloud to be practical on 1 PB servers, the first copy of data needs to move at rates far exceeding 1 gbps. When evaluating existing software, it becomes evident that any backup strategy that necessitates routine \u0026ldquo;full\u0026rdquo; backups of these kinds of servers are impractical. Although seeding a large backup can take significant time (weeks in extreme cases) and is unavoidable, ongoing backup needs to be \u0026ldquo;forever incremental.\u0026rdquo;\nThe second metric (checks/sec) is not so obvious at first glance. To synchronize the contents of two file \u0026ldquo;systems,\u0026rdquo; the backup software must verify that everything that is local is on the backup. Furthermore, everything that has been deleted from the local must also be deleted from the backup. Rclone performs this with \u0026ldquo;checkers\u0026rdquo; that check whether the local and cloud copies are in sync. Because each check is an over-the-network API call with tens of milliseconds per call, many such checks must occur in parallel. Rclone does this natively and UCI routinely observes approximately 2000 checks/second.\nAlternatively, an organization could keep a local database of the state of each file and reduce the regular \u0026ldquo;sync.\u0026rdquo; That\u0026rsquo;s a tradeoff between space (a local database) and time (weekly sync check). Many commercial backup programs do this (and must always have a way of re-establishing truth if the local database is corrupted). The management of such a database increases complexity.\nE. Low Cost Cost optimization was a key requirement, necessitating careful analysis of AWS feature usage for cost containment. UCI had to understand some of the basics of how rclone interfaces with Amazon S3, and then do the multiplication. After examining practical performance issues, UCI settled on weekly \u0026ldquo;deep syncs\u0026rdquo; of file systems. The deep syncs resulted in a large number of API calls and tax the local file system. Rclone has a mode called a \u0026ldquo;top up\u0026rdquo; sync. It only considers files that are new or newly written for comparison to the cloud copy. Systems run top-ups six days/week and a deep sync one day/week.\nF. Password Rotation and Detecting Non-Running The backup script (gen-backup.py) must be able to run unattended from a task scheduler (for example cron on Linux-like systems, Task Scheduler on Windows). To do this, rclone must have access to valid credentials. This provides somewhat of an engineering conundrum: temporary (short-lived) keys might expire before a backup is completed while long-lived credentials pose their own risks. Rclone to S3 for large backup (RCS3) \u0026ldquo;splits the middle\u0026rdquo;: credentials (specific to the backup process itself) are long-lived but are expired after the completion of every successful invocation of the backup program (gen-backup.py). Under normal operation, this occurs daily.\nWhen a bucket is set up for backup, a service account (backup user) is created and this account must have the privilege to create a new access key/secret access key pair just for itself. This compromise works very nicely. When rclone starts, the key being used doesn\u0026rsquo;t expire while the backup is ongoing, thus preventing wasteful restarts of partially-completed backups. When completed, the key that was just used is expired and a new one is created. Under normal operations the key has a lifetime of 24 hours.\nOne can use the fact that normal operations should see a key update every day. This triggers an alarm if a particular key has an age of more than 48 hours. This indicates that either: a backup is taking longer or a backup isn\u0026rsquo;t getting to the key rotation step. The latter case is always a problem. The root cause of the alarm necessitates the sysadmin to investigate.\nConclusion This comprehensive backup solution developed by UCI RCIC demonstrates how AWS services can be effectively leveraged to address the complex challenge of backing up massive amounts of research data across numerous faculty labs. Through the strategic implementation of rclone, custom Python wrappers, and various AWS services including S3, IAM, CloudWatch, and SNS, UCI successfully created a scalable system capable of backing up over 5 PB of data comprising more than 250 million files.\nThe solution\u0026rsquo;s architecture effectively balances critical requirements including cost efficiency, performance optimization, security controls, and monitoring capabilities while maintaining a clear separation between system and cloud administration roles.\nKey Benefits Cost Savings: Intelligent storage tiering and lifecycle policies Security: Segregation of duties and strict access controls Monitoring: Comprehensive capabilities with both high-level and detailed per-server insights Scalability: Handles servers ranging from 1 TB to 2 PB while maintaining system performance For organizations looking to implement a similar solution, UCI\u0026rsquo;s approach demonstrates that with careful planning and the right combination of tools and services, even the most demanding backup requirements can be met effectively.\nRecommended Resources UCI\u0026rsquo;s public GitHub repository RCS3 Documentation The documentation provides an overview of the two administrative sides of backup, and what is needed in terms of software (Python, Boto3 library, and rclone), initializing the cloud environment for backup, onboarding servers, creating backup jobs, defining quotas, and updating dashboards.\nFuture Roadmap RCS3 has demonstrated the effectiveness of managing Petabyte-scale backups while maintaining cost efficiency. Our near-term to do list includes:\nUnassisted (no cloudadmin intervention) restore operations Documentation of various backup job options that can deal with common issues Details on setting up all the system variants that have been encountered so far About the Authors Philip Papadopoulos Philip Papadopoulos is the Director of the Research Cyberinfrastructure Center (RCIC) at the University of California, Irvine (UCI). RCIC serves the campus research community via a mid-scale (roughly 11K cores and 150 GPUs) computing cluster and about 10 PB of parallel storage. Dr. P. started working in the parallel/distributed computing community at Oak Ridge National Laboratory as part of the PVM (precursor to MPI) in the late 1990s. Prior to UCI, he spent nearly twenty years at the San Diego Supercomputer Center building supercomputing systems and developing the Rocks cluster toolkit.\nAbhijeet Lokhande Abhijeet Lokhande is a Senior Solutions Architect at AWS, where he plays a pivotal role in empowering India\u0026rsquo;s most innovative startups. Working within the AWS Startups team, he serves as a strategic advisor, helping emerging companies transform their ambitious visions into scalable cloud architectures. With deep expertise in security and compliance, Abhijeet guides founders through the complexities of building secure, enterprise-grade applications on AWS.\nEvan Wood Evan is a Solutions Architect working with the Amazon Web Services (AWS) Worldwide Public Sector team. He works with the Department of Energy in the Federal Civilian space, helping them accelerate their Cloud journey. Evan specializes in IoT deployments and databases.\nFrancisco Ramon Lopez Francisco is an IT professional working in the Research Cyberinfrastructure Center (RCIC) at the University of California, Irvine. He has 25+ years of experience with UNIX systems and began working with AWS in 2015.\nNicholas Santucci Nick Santucci is a High-Performance Computing (HPC) Systems Engineer at UC Irvine\u0026rsquo;s Research Cyberinfrastructure Center (RCIC), part of the Office of Data and Information Technology (ODIT). He specializes in software integration, cluster computing, and high-performance storage, supporting the RCIC\u0026rsquo;s mission to provide scalable computing and storage resources for the campus research community. Before joining UC Irvine to work in high performance computing/research computing, he spent a decade at DIRECTV, where he developed and migrated automated monitoring systems for broadcast operations, shifting the practice from tool-driven monitoring to data-driven IT analytics. Nick holds a bachelor\u0026rsquo;s degree in information and computer science from UC Irvine, with a specialization in network and distributed systems.\nTags: Amazon Athena, Amazon CloudWatch, Amazon DynamoDB, Amazon EventBridge, Amazon S3 Batch Operations, Amazon S3 Glacier Storage Classes, Amazon Simple Storage Service (Amazon S3), Amazon SNS, AWS Cloud Storage, AWS Lambda\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Ngo Van Minh Tri\nPhone Number: 0965420205\nEmail: ngovanminhtri05@gmail.com\nUniversity: FPT University HCM Campus\nMajor: Software Engineer\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog: Smart Office - Rules Engine &amp; Data Persistence","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 10 Objectives: Implement Event-Driven Logic: Use AWS IoT Rules Engine to filter and route incoming MQTT messages without managing servers. Data Persistence: Configure an Amazon DynamoDB integration to store time-series sensor data (Temperature, Humidity) for historical analysis. Automated Alerting: Set up an Amazon SNS topic to trigger immediate notifications when sensor readings exceed defined thresholds (e.g., Fire/High Temp). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Database Design (NoSQL): + Create a DynamoDB table SmartOffice_Telemetry. + Partition Key: device_id (String). + Sort Key: timestamp (Number/Epoch). + Goal: Optimize for time-series queries. 11/10/2025 11/10/2025 Best Practices for DynamoDB Time-Series 3 - Configure IoT Rule (Persistence): + Write SQL query: SELECT * FROM 'smart-office/+/data'. + Add Action: Insert message into DynamoDB table. + Create IAM Role granting IoT Core permission to PutItem. 11/11/2025 11/11/2025 Creating an AWS IoT Rule 4 - Configure IoT Rule (Alerting): + Create SNS Topic Office_Alerts and subscribe email. + Write SQL query with Condition: SELECT * FROM 'smart-office/+/data' WHERE temperature \u0026gt; 30. + Add Action: Send message to SNS. 11/12/2025 11/12/2025 IoT Rule SQL Reference 5 - End-to-End Testing: + Run the Device Simulator (from Week 9). + Send \u0026ldquo;Normal\u0026rdquo; data -\u0026gt; Verify entry in DynamoDB. + Send \u0026ldquo;High Temp\u0026rdquo; data -\u0026gt; Verify DynamoDB entry AND receive Email Alert. 11/13/2025 11/13/2025 Self-Verification Week 10 Achievements: Built a Serverless Backend for IoT:\nLeveraged AWS IoT Rules Engine to decouple the device layer from the storage layer. No EC2 instances were used to process the data, reducing operational overhead to zero. Established Data Persistence:\nSuccessfully routed telemetry data into Amazon DynamoDB. Validated the schema design (device_id + timestamp), ensuring data is stored efficiently for future dashboard retrieval. Implemented Real-time Anomaly Detection:\nConfigured conditional logic (WHERE temperature \u0026gt; 30) directly at the ingestion layer. Verified that critical alerts (SNS) are triggered instantly, while normal data is simply logged, optimizing cost and noise. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog: Smart Office - Serverless API &amp; Visualization","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 11 Objectives: Build the Logic Layer: Develop an AWS Lambda function to retrieve and format historical sensor data from DynamoDB. Expose via REST API: Configure Amazon API Gateway to create a public-facing, secure endpoint for the application. Expand Cloud Knowledge: Participate in the AWS Cloud Mastery Series #2 to deepen understanding of advanced architectural patterns. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Participate in event AWS Cloud Mastery Series #2 + Topic: Advanced Serverless Patterns / Resilience. + Key Takeaway: Learned about idempotency and cold starts. 11/07/2025 11/07/2025 Event Recording / Notes 3 - Develop Backend Logic (Lambda): + Create a Lambda function (Node.js/Python). + Attach IAM Role with dynamodb:Scan or dynamodb:Query permissions. + Write code to fetch the last 10 readings from the SmartOffice_Telemetry table. 11/18/2025 11/18/2025 Building Lambda functions with DynamoDB 4 - Configure API Gateway: + Create a REST API. + Create a Resource /data and Method GET. + Integrate the Method with the Lambda function (Proxy Integration). 11/19/2025 11/19/2025 Build a REST API with Lambda proxy integration 5 - Security \u0026amp; CORS: + Enable CORS (Cross-Origin Resource Sharing) on API Gateway to allow browser access. + Deploy the API to a Stage (e.g., dev). + Test the Invoke URL using Postman or curl. 11/20/2025 11/20/2025 Enabling CORS for a REST API resource 6 - Visualization (Frontend POC): + Update the S3 Static Website (from Week 4/7). + Add a simple JavaScript fetch() call to the new API Endpoint. + Display the live temperature data on the webpage. 11/21/2025 11/21/2025 Self-Verification Week 11 Achievements: Developed Serverless Business Logic:\nSuccessfully wrote and deployed an AWS Lambda function to bridge the gap between the storage layer (DynamoDB) and the user. Implemented IAM Least Privilege by granting the function only Read access to the specific telemetry table. Created a Secure REST Interface:\nDeployed Amazon API Gateway to expose the IoT data over standard HTTP(S). Enabled CORS, resolving common browser-security issues when connecting a frontend (S3) to a backend API. Continuous Learning:\nActively participated in AWS Cloud Mastery Series #2, gaining insights into industry-standard cloud patterns which I applied to the current Smart Office project. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog: AWS Foundations and Command Line Mastery","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Master the fundamentals of core AWS service categories to understand platform capabilities. Establish secure, automated resource interaction via the AWS Console and Command Line Interface (CLI). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 Internal Onboarding Docs 3 - Learn about AWS and its types of services: + Compute: EC2, Lambda, ECS + Storage: S3, EBS, EFS + Networking: VPC, Route 53 + Database: RDS, DynamoDB 08/12/2025 08/12/2025 AWS Cloud Concepts Overview 4 - Create AWS Free Tier account with IAM Best Practices (MFA, Admin User). - Install \u0026amp; configure AWS CLI to enable automation. - Practice: + Configure CLI profiles (aws configure). + Verify access (aws sts get-caller-identity). 08/13/2025 08/13/2025 AWS Free Tier Guide Install AWS CLI 5 - Learn basic EC2 components: + Instance Types: (General Purpose vs. Compute Optimized) + AMI: Customizing OS images + EBS: Volume types (gp3, io2) \u0026amp; snapshots - Security: SSH Key Pairs \u0026amp; Security Groups. 08/14/2025 08/15/2025 Amazon EC2 User Guide 6 - Practice: + Launch an EC2 instance via Console. + Connect via SSH (ssh -i key.pem ec2-user@IP). + Attach an EBS volume and mount it. 08/15/2025 08/15/2025 Tutorial: Launch EC2 Instance Week 2 Achievements: Established Foundational AWS Security: Successfully created an AWS Free Tier account and immediately implemented IAM best practices by creating a dedicated administrative user with Multi-Factor Authentication (MFA), rather than using the root account.\nMastered Core AWS Service Categories: Gained a working understanding of the foundational services (Compute, Storage, Networking, Database) and their roles in creating resilient, scalable cloud architecture.\nInsight: Recognized that S3 is Object storage suitable for static assets, while EBS is Block storage required for EC2 OS drives. Achieved Operational Automation via AWS CLI: Installed, configured, and leveraged the AWS CLI (Command Line Interface).\nConfigured credentials using aws configure. Verified successful authentication using aws sts get-caller-identity. DevOps Note: CLI mastery is the first step toward scripting and full Infrastructure as Code (IaC). Implemented and Validated EC2 Deployment: Successfully launched, configured, and terminated an EC2 instance.\nPracticed creating Security Groups (firewall rules) to allow SSH traffic on port 22 only from my specific IP. Attached and mounted an EBS volume to a running Linux instance to simulate expanding server storage dynamically. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: AWS Foundations and Command Line Mastery\nWeek 3: Architecting VPC \u0026amp; Secure Compute\nWeek 4: Managed Databases \u0026amp; Storage\nWeek 5: High Availability \u0026amp; Auto Scaling\nWeek 6: Monitoring \u0026amp; Observability\nWeek 7: Content Delivery \u0026amp; Edge Security\nWeek 8: Review knowledge for mid-term test\nWeek 9: Project Kickoff - Smart Office IoT Core\nWeek 10: Smart Office - Rules Engine \u0026amp; Data Persistence\nWeek 11: Smart Office - Serverless API \u0026amp; Visualization\nWeek 12: Final project completion and presentation\nWeek 13: Coursera AWS Certification\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Introduction to Foundation Models: Understanding the core concepts of FMs. Prompt Engineering: A guide to effective prompting techniques. Amazon Bedrock Exploration: Deep dive into Generative AI using Amazon Bedrock. RAG Architecture: Understanding Retrieval-Augmented Generation and Knowledge Base integration. Speakers Danh Hoang Hieu Nghi Lam Truong Kiet Dinh Le Hoang Anh Key Highlights Prompting Techniques Strategies to craft effective prompts that yield more accurate and context-aware results. RAG (Retrieval-Augmented Generation) Enhancing AI capabilities by embedding external data sources. Enables the model to respond with accurate, real-time information derived from internal/proprietary data. Embeddings Definition: The numerical representation of text (vectors) that captures semantics and relationships between words. Functionality: Embedding models capture the features, context, and nuances of the input text. Comparison: Rich embedding models allow for semantic text similarity comparisons. Multilingual Support: Capable of identifying meaning and relationships across different languages. featured AWS AI Services Vision \u0026amp; Document Analysis: Amazon Rekognition, Amazon Textract, Amazon Lookout. Language \u0026amp; Speech: Amazon Translate, Amazon Transcribe, Amazon Polly, Amazon Comprehend. Search \u0026amp; Data: Amazon Kendra, Amazon Personalize. Key Takeaways Prompting Techniques - Chain of Thought Contextual Examples: Providing the AI with specific use-case examples (few-shot prompting). Step-by-Step Logic: Instructing the AI to break down the problem-solving process into sequential steps to improve reasoning. RAG Use Cases Accuracy: Reduces hallucinations by grounding responses in recent, verified enterprise data. Chatbot Enhancement: Improves chatbot capabilities by integrating real-time data access. Personalization: Enables search functionality based on user history and specific personas. Data Extraction: Retrieving and summarizing specific transactional details from large datasets. Amazon Titan Embeddings Function: Translates text inputs (words, phrases) into numerical vectors. Advantage: Comparing embeddings produces significantly more relevant and context-aware responses than simple keyword matching. Specs: Max Tokens: 8,000 Output Vectors: 256, 512, 1,024 Language Support: Multilingual (100+ languages in preview) Applying to Work I plan to investigate additional AWS AI Services to determine their viability for integration into future projects. Event Experience Attending the “AI/ML/GenAI on AWS” workshop was an extremely valuable experience. I had the opportunity to acquire new technical knowledge and network with other IT professionals. Key experiences included:\nInsights from Industry Experts Experts from FACJ shared best practices for deploying AI in production environments. Through real-world case studies, I gained a deeper understanding of how to apply advanced prompting techniques effectively. Exploring AWS AI Services Through speaker demonstrations, I learned the mechanics of various AWS AI services and observed their application in real-life scenarios. Guidance on Building Agents I gained practical insights and foundational knowledge required to start building AI Agents. Overall: This event provided me with significant knowledge regarding AI that is directly applicable to real-world projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Serverless \u0026amp; Event-Driven Architecture Serverless Architecture: This workshop utilizes a cloud-native model with services like AWS Lambda, Amazon API Gateway, and Amazon DynamoDB. This approach allows code to run in response to requests without provisioning or managing servers, as AWS handles all automatic scaling and infrastructure management. Event-Driven Architecture: The core of the system functions on an event-driven basis. Instead of services continuously polling for data, specific events—such as IoT sensor readings or user API calls—trigger downstream workflows. This is orchestrated by AWS IoT Core and Amazon EventBridge, creating a highly flexible and scalable system. Workshop overview In this workshop, you will deploy a comprehensive serverless data platform on AWS to manage real-time environmental monitoring for an 8-room smart office setup. The system integrates AWS IoT Core, Lambda, DynamoDB, S3, CloudFront, and Amazon Cognito. Sensor data is forwarded from edge devices (or simulated scripts), ingested into AWS, stored in DynamoDB tables, and processed by Lambda functions to update the management dashboard. Critical events are routed through EventBridge for alerting, demonstrating a high-availability, low-cost, and seamless scalability architecture.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Guide to IP and Domain Warming and Migrating to Amazon SES","tags":[],"description":"","content":"Guide to IP and Domain Warming and Migrating to Amazon SES By Tyler Holmes | 03 JUL 2025\nCategories: Amazon Simple Email Service (SES), Messaging\nIntroduction Transitioning your email workloads from another email service provider (ESP) to Amazon Simple Email Service (Amazon SES) can be a challenge, given that each workload can be unique. In this post, we show you how to successfully warm up IP addresses and domains when migrating to Amazon SES. This guide aims to provide a comprehensive overview of IP and domain warming best practices so you can make your transition to Amazon SES as smooth as possible. We discuss some of the challenges you might encounter and how to overcome those common pitfalls when transitioning to a new email service provider (ESP).\nUnderstanding IP and Domain Email Warming IP warming and domain warming are strategic processes designed to gradually introduce a new sending identity to mailbox providers. A new sending identity can be a dedicated IP address, new domain, a subdomain of that domain, or any combination of them. The core objective of warming is to build a positive reputation with mailbox providers so your emails are delivered to the inbox rather than being filtered into spam folders or potentially blocked from being delivered to a mailbox altogether.\nMailbox providers such as Gmail, Yahoo, and Outlook are vigilant about protecting their users from spam and malicious content. When you introduce a new sending identity, mailbox providers evaluate the new sending identity with caution. They evaluate the early sending from the domain and IPs to ensure they\u0026rsquo;re sending the mailbox provider\u0026rsquo;s users messages that are wanted and aren\u0026rsquo;t engaged in abusive practices such as spam or phishing. Warming provides mailbox providers the opportunity to observe your sending patterns, content, and engagement metrics, allowing them to gradually build trust in your new sending identity.\nWarming can be different for each scenario. For example, you can have completely warmed IPs, but if your sending domain is new, you\u0026rsquo;ll likely have to warm it up as well but you won\u0026rsquo;t need to worry about IP warming as much. Another common scenario is that of adding a new IP but sending with an established domain. In this case, the IP will need warming, but the domain itself is helping the warming because it already has an established reputation. When you have a net new IP and a net new domain, you\u0026rsquo;ll have to warm them together. The warm-up best practices we outline in this post, such as starting out slow and targeting your highest engaged subscribers first, apply to your situation.\nWhy Warming is Critical Warming is essential for several reasons, each contributing to the overall success and reliability of your email marketing campaigns:\n1. Building Trust with Mailbox Providers A positive sender reputation is crucial for email deliverability. Mailbox providers use complex algorithms to evaluate the reputation of senders, and warming helps them build trust in your new sending identity.\n2. Avoiding Initial Deliverability Issues When you switch to a new ESP or introduce a new sending identity, it\u0026rsquo;s common to experience an initial dip in deliverability metrics, such as lower open rates, click-through rates, or higher bounce rates. Warming can mitigate these issues by giving mailbox providers time to adapt to your new sending patterns, whether that is a new domain, subdomain, or new IP infrastructure.\n3. Maintaining Consistent Sending Behavior Warming encourages you to maintain a steady and predictable sending cadence. Sudden, significant changes in sending volume, content, or frequency can trigger inbox spam filters because such changes might indicate that the sender has been compromised or is engaging in abusive practices. Even anomalies such as large changes in volume or throughput during seasonal events such as Black Friday can be interpreted as negative, and mailbox providers take a cautious approach when they detect anomalies such as sudden large spikes in volume.\n4. Long-term Deliverability Success It\u0026rsquo;s a misconception that warming is done only one time. In reality you need to maintain traffic volumes and sending cadences to keep those sending identities warm. Additionally, if you plan on increasing volume considerably, for example from 1M to 5M or 5M to 25M, you need to warm up to those volumes. Those large jumps in volumes look suspicious to inbox providers even if you\u0026rsquo;ve been sending consistently.\n5. Adapting to Mailbox Provider Changes Mailbox providers also continuously update their algorithms to better detect spam and abusive behavior. If you view warming as a constant process and consistently monitor your deliverability and engagement signals you can make adjustments to your sending strategy as needed, ensuring that your emails continue to reach the inbox, even as inbox and audience behavior changes.\nCommon Challenges to Moving Traffic and Warming Up on a New ESP Transitioning your email traffic to a new ESP can present unique challenges that require careful consideration and strategic planning to overcome. These challenges include the following:\nEvent-driven traffic – If all your email is event-driven, it\u0026rsquo;s hard to control volume and throughput.\nMultiple sending domains – Having many sending domains with varying traffic volumes and throughputs can complicate the transition.\nNo shared IPs – Some organizations aren\u0026rsquo;t allowed to use shared pools of IPs.\nLack of engagement data – Absence of data related to engagement can make it difficult to optimize the warming process.\nOutdated bounce and unsubscribe info – Not having up-to-date bounce and unsubscribe information in your current ESP can lead to deliverability issues.\nSingle second-level domain – You\u0026rsquo;re currently sending your mail from your second-level domain, such as example.com, without separate subdomains for logical use cases such as transactional or marketing.\nTight timelines – Contracts ending or other reasons might impose a tight timeline for the transition.\nChallenges for independent service providers (ISVs) and software-as-a-service (SaaS) providers – These organizations often don\u0026rsquo;t have complete control over the volume, content, lists, or sending consistency of their customers. They also might not have direct access to the DNS needed to update and align sending domains and authentication.\nStrategies for a Successful Warm-up and Migration The following list isn\u0026rsquo;t suitable for every case, and many customers will use more than one strategy to address their challenges and smooth their transition to Amazon SES:\n1. Send to Your Best Audience First The most important thing you need to do when transitioning to a new ESP is to send to your highest and most active recipients on the new ESP and leave the less active or risky segments on the previous ESP until you\u0026rsquo;re ready to full switch. For example, if you\u0026rsquo;re a daily sender who sends to 1M addresses a day and have an open rate of about 20%, you need to start onboarding with segments that include those who are opening. A good strategy is to start with openers from the last 30 days, then move to openers from 31–90 days, and so on.\n2. Gradually Move Your Less Engaged Subscribers After you\u0026rsquo;ve transitioned your most active segments, you can start to include the less engaged a little at a time. You can sprinkle them in with the more engaged segments so that if you do get bounces or complaints it is buffered by the more engaged segments. Make sure to continue monitoring for issues and immediately stop increasing your workloads if you encounter deliverability issues such as increased bounce or spam rates.\n3. Start with Predictable Workloads Begin with workloads that aren\u0026rsquo;t time-dependent, such as newsletters, which are easier to control and monitor.\n4. Batch Event-driven Messages For event-driven messages that aren\u0026rsquo;t time-sensitive, try to batch and spread them out to manage the volume.\n5. Use Automated Warm-up Processes Standard and managed dedicated IPs can help to manage the daily volume by allowing predefined levels of traffic on your dedicated IPs and spilling over into shared IP pools when the volume of email has reached a level we deem to be sufficient for your dedicated IPs. This is dependent on your warm-up progress thus far. Dedicated standard is a static 45-day increase, but managed dedicated has a more sophisticated process. To learn more, refer to Dedicated IP addresses for Amazon SES.\n6. Strategically Use Shared IP Pools Use shared IP pools for workloads that don\u0026rsquo;t require dedicated IPs. Because there is consistent volume already going through these IPs, they\u0026rsquo;re a little more forgiving than dedicated IPs being warmed up.\n7. Transition Gradually to Dedicated IPs Begin with shared IPs and gradually transition to dedicated IPs as they warm up.\n8. Transition Gradually to Logical Subdomains Split your traffic into logical workloads that can have consistent volume and throughput. Even something as simple as marketing.example.com and transactional.example.com is better than sending mail from example.com\n9. Onboard New Customers on the New ESP For ISVs and SaaS providers, consider onboarding new customers directly on the new ESP to gather initial data and test the waters. New customers already need to be warmed up, so if you warm them up on Amazon SES rather than your legacy ESP, you don\u0026rsquo;t need to go through a warming process twice.\nPrepare to Migrate Email Traffic to Amazon SES Before you migrate your email program to Amazon SES, it\u0026rsquo;s important to thoroughly document and organize your existing setup. This preparatory work will lay the foundation for a successful warm-up and migration process.\nPreparation Checklist Document your use cases – Categorize your use cases as either marketing or transactional. This will help you understand the nature of the emails you send and how they should be handled.\nDocument your sending domains – Include the \u0026ldquo;from\u0026rdquo; names associated with each domain. This will assist in mapping the appropriate domain to the corresponding email type. Ideally, you should avoid sending from your root domain. For example, use a subdomain such as email.brand.com instead of brand.com. Review and document your authentication (for example, SPF, DKIM, or DMARC). In some cases, you might not need to align all of them, but you\u0026rsquo;ll definitely need to align DMARC as part of the bulk sender requirements.\nMap use cases to sending domains and from names – Create a clear correspondence to ensure the right emails are sent from the appropriate domains. At a minimum, it\u0026rsquo;s a best practice to have separate subdomains for transactional and promotional email use cases, such as transactional.brand.com and promo.brand.com.\nDocument volume and max throughput – Capture this information for each use case mapped to your sending domains. This will help you understand the scale of your email operations and plan your architecture and warming strategy accordingly.\nAnticipate a temporary dip in deliverability metrics – While transitioning to a new ESP, you might experience a short-term fluctuation in metrics such as open rates and click-through rates. This is a common occurrence and shouldn\u0026rsquo;t be viewed as a failure of the service. It\u0026rsquo;s an expected part of the migration process as mailbox providers adapt to your new sending identity. By closely monitoring your bounce and complaint rates, you can make proactive adjustments to your ramp-up plan to ensure a smooth transition.\nDocument your warming plan – Have a plan to gradually increase traffic for each identity and monitor engagement metrics. Plan for how to address high bounce or complaint rates.\nSample Warm-up Plan The following table shows a sample warm-up plan. Notice that the days are categorized by large inbox providers. This is because these providers all accept new mail at different rates. Categorizing this way is a recommended best practice, but if you can\u0026rsquo;t segment that granularly, then you can use the Daily totals column as a guide. The AWS managed dedicated IP service automatically does this segmentation and throttling at the domain level for you.\nThe following plan is a typical ramp. You can get more aggressive the higher your overall engagement rates are, so if you\u0026rsquo;re at 40–60% engagement, you can use this warm-up. If your rates are lower, you might want to be a little more conservative. Make sure to be adaptive as you go into your warming plan because you might need to maintain the same rate for a couple days or even roll back a step if you\u0026rsquo;re experiencing negative trends such as a drop in deliverability or engagement. Remember, as you get into the less engaged segments of your list, engagement will drop, but it shouldn\u0026rsquo;t be drastic. Constantly monitor your metrics during this critical time.\nDay @gmail.com @hotmail.com @outlook.com @yahoo.com @icloud.com @aol.com Others Daily Total 1 150 150 150 150 150 150 150 1,050 2 300 300 300 300 300 300 300 2,100 3 600 600 600 600 600 600 600 4,200 4 1,200 1,200 1,200 1,200 1,200 1,200 1,200 8,400 5 2,400 2,400 2,400 2,400 2,400 2,400 2,400 16,800 6 5,000 5,000 5,000 5,000 5,000 5,000 5,000 35,000 7 10,000 10,000 10,000 10,000 10,000 10,000 10,000 70,000 8 20,000 20,000 20,000 20,000 20,000 20,000 20,000 140,000 9 40,000 40,000 40,000 40,000 40,000 40,000 40,000 280,000 10 80,000 80,000 80,000 80,000 80,000 80,000 80,000 560,000 11 150,000 150,000 150,000 150,000 150,000 150,000 150,000 1,050,000 12 300,000 300,000 300,000 300,000 300,000 300,000 300,000 2,100,000 13 425,000 425,000 425,000 425,000 425,000 425,000 425,000 2,975,000 14 500,000 500,000 500,000 500,000 500,000 500,000 500,000 3,500,000 15 600,000 600,000 600,000 600,000 600,000 600,000 600,000 4,200,000 16 650,000 650,000 650,000 650,000 650,000 650,000 650,000 4,550,000 17 700,000 700,000 700,000 700,000 700,000 700,000 700,000 4,900,000 18 800,000 800,000 800,000 800,000 800,000 800,000 800,000 5,600,000 19 900,000 900,000 900,000 900,000 900,000 900,000 900,000 6,300,000 20 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 1,000,000 7,000,000 21 1,100,000 1,100,000 1,100,000 1,100,000 1,100,000 1,100,000 1,100,000 7,700,000 22 1,200,000 1,200,000 1,200,000 1,200,000 1,200,000 1,200,000 1,200,000 8,400,000 23 1,300,000 1,300,000 1,300,000 1,300,000 1,300,000 1,300,000 1,300,000 9,100,000 24 1,400,000 1,400,000 1,400,000 1,400,000 1,400,000 1,400,000 1,400,000 9,800,000 25 1,500,000 1,500,000 1,500,000 1,500,000 1,500,000 1,500,000 1,500,000 10,500,000 26 1,600,000 1,600,000 1,600,000 1,600,000 1,600,000 1,600,000 1,600,000 11,200,000 27 1,700,000 1,700,000 1,700,000 1,700,000 1,700,000 1,700,000 1,700,000 11,900,000 28 1,800,000 1,800,000 1,800,000 1,800,000 1,800,000 1,800,000 1,800,000 12,600,000 29 1,900,000 1,900,000 1,900,000 1,900,000 1,900,000 1,900,000 1,900,000 13,300,000 30 2,000,000 2,000,000 2,000,000 2,000,000 2,000,000 2,000,000 2,000,000 14,000,000 31 2,100,000 2,100,000 2,100,000 2,100,000 2,100,000 2,100,000 2,100,000 14,700,000 32 2,200,000 2,200,000 2,200,000 2,200,000 2,200,000 2,200,000 2,200,000 15,400,000 33 2,300,000 2,300,000 2,300,000 2,300,000 2,300,000 2,300,000 2,300,000 16,100,000 34 2,400,000 2,400,000 2,400,000 2,400,000 2,400,000 2,400,000 2,400,000 16,800,000 35 2,500,000 2,500,000 2,500,000 2,500,000 2,500,000 2,500,000 2,500,000 17,500,000 36 2,600,000 2,600,000 2,600,000 2,600,000 2,600,000 2,600,000 2,600,000 18,200,000 37 2,700,000 2,700,000 2,700,000 2,700,000 2,700,000 2,700,000 2,700,000 18,900,000 38 2,800,000 2,800,000 2,800,000 2,800,000 2,800,000 2,800,000 2,800,000 19,600,000 39 2,900,000 2,900,000 2,900,000 2,900,000 2,900,000 2,900,000 2,900,000 20,300,000 40 3,000,000 3,000,000 3,000,000 3,000,000 3,000,000 3,000,000 3,000,000 21,000,000 Best Practices for a Successful IP Warm-up A successful IP warm-up involves a strategic approach that combines technical preparation, engaged subscribers, compelling content, and ongoing monitoring.\n1. Ensure Technical Readiness Configure DNS records and set up SPF, DKIM, DMARC, and BIMI so your email content complies with best practices. Make sure your DMARC is aligned if you\u0026rsquo;re sending across multiple ESPs or applications.\n2. Use an Engaged, Permission-based Mailing List Use a clean, opt-in list of subscribers who are interested in your content. For more information, refer to Optimizing Email Deliverability: A User-Centric Approach to List Management and Monitoring.\n3. Provide Compelling, Valuable Email Content Send content that resonates with your audience and encourages engagement.\n4. Gradually Ramp Up Sending Volume and Cadence Start with a small volume of emails and gradually increase over time to allow mailbox providers to observe your sending patterns.\n5. Maintain Consistency in Sending Behavior Avoid sudden, significant changes in sending volume, content, or frequency.\n6. Continuously Monitor and Optimize Key Metrics Track open rates, click-through rates, bounce rates, and complaint rates, and make adjustments as needed. For more information, refer to Amazon SES – Set up notifications for bounces and complaints.\n7. Ongoing Maintenance of Sender Reputation Audit data flows, ramp up changes gradually, and follow evolving email marketing best practices.\nNavigating Initial Deliverability Challenges When transitioning to a new ESP, you might encounter some initial deliverability challenges. It\u0026rsquo;s important to monitor and exercise caution if you observe increased bounce or complaint rates. If you do have challenges, you need to address them promptly. Maintain the same volume or even reduce the volume the next day if you encounter these issues:\n1. Spike in Hard Bounce Rates Bring over your suppression lists from the ESP you\u0026rsquo;re offboarding from, but if your previous ESP didn\u0026rsquo;t manage these well or you load some old addresses you weren\u0026rsquo;t aware of, it\u0026rsquo;s common to experience hard bounce spikes at the beginning. If this happens, slow your volume increases or even stop increasing until things stabilize. It\u0026rsquo;s more important to warm up properly than it is to get to production levels of sending as fast as possible. This is one more reason that it\u0026rsquo;s always best to start with your most engaged segments.\n2. Increased Spam Complaints Emails might reach recipients who previously filtered them, leading to more spam complaints. Changing an identity can also cause your recipients to hit the spam button because they don\u0026rsquo;t recognize it. Announce identity changes before changing your ESP to reduce the chances of an issue.\n3. Heightened Mailbox Provider Scrutiny Mailbox providers will closely monitor new senders to confirm they\u0026rsquo;re not engaged in malicious activities. This can divert emails to the spam filter initially or even be throttled if you reach volume or throughput limits. Gmail is known to be stringent. Amazon SES managed dedicated IPs use our data to know how much mail the big inbox providers will accept while you\u0026rsquo;re warming up and keep you from overshooting their limits.\n4. ESP Throttling and Sending Limits The new ESP might have stricter rules regarding the volume of emails that can be sent to individual mailbox providers. Amazon SES has account limits for daily volume and max throughput, so adjust yours to what you\u0026rsquo;ll need. To learn more, refer to Increasing your Amazon SES sending quotas in the Amazon SES Developer Guide.\nMaintaining IP Reputation After Warming IP warming is an ongoing process. Even after the initial warm-up phase, it\u0026rsquo;s essential to maintain your sender reputation by continuously managing your email program. Your subscriber engagement might fluctuate as your list grows and changes. Similarly, ramping up email volume for a seasonal campaign will require adjustments to your warm-up process. You need to be proactive and adapt your IP warming strategy.\nAudit data flows and campaigns and monitor email list sources, data collection practices, and campaign performance. When introducing new elements, do so incrementally to avoid triggering reputation issues. To allow time for your reputation to stabilize, provide at least a month for a new baseline to be established after major program changes. Engage with customers, provide value, and implement re-engagement campaigns to nurture your customer relationships. Adhere to evolving email marketing best practices, including proper authentication protocols and emerging technologies. Be proactive and track domain and IP reputation so you can quickly address the deliverability issues that arise. To learn more about monitoring inbox tools such as Google Postmaster, refer to Understanding Google Postmaster Tools (spam complaints) for Amazon SES email senders.\nConclusion Transitioning your email program to a new ESP such as Amazon SES can seem complex, but it can be quick and seamless if you follow the best practices explained in this post. IP warming is a critical component of this process because it helps build a positive sender reputation with mailbox providers and promotes the reliable delivery of your emails.\nThroughout this guide, we\u0026rsquo;ve covered the key aspects of IP warming and email migration, from understanding the importance of this practice to identifying common challenges and outlining effective strategies for a successful transition. By following best practices such as facilitating technical readiness, using an engaged subscriber base, providing compelling content, and gradually ramping up sending volume, you can navigate the initial deliverability challenges and establish a strong foundation for long-term email program success.\nHowever, the work doesn\u0026rsquo;t stop when the initial warm-up phase is complete. Maintaining IP reputation and adapting your strategy as your email program and subscriber engagement evolve is an ongoing process. Continuously monitoring key metrics, auditing data flows, and staying up to date with evolving email marketing best practices is crucial for sustaining deliverability. A long-lasting sender reputation and enduring relationships with your list and recipients are some of the key benefits of following these best practices. Transitioning to a new ESP is a significant undertaking, but with the right preparation, execution, and commitment to ongoing maintenance, your migration can be smooth and successful.\nAbout the Author Tyler Holmes Tyler is a Senior Specialist Solutions Architect. He has a wealth of experience in the communications space as a consultant, an SA, a practitioner, and leader at all levels from Startup to Fortune 500. He has spent over 14 years in sales, marketing, and service operations, working for agencies, consulting firms, and brands, building teams and increasing revenue.\nResources for Deliverability Amazon SES Developer Guide Amazon SES API documentation Service quotas in Amazon SES Amazon SES endpoints and quotas Amazon SES pricing Amazon Pinpoint Getting Started with Amazon Pinpoint Tags: Amazon SES, Email Deliverability, IP Warming, Domain Warming, Email Migration, Email Marketing\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Event Objectives Cultivating the DevOps Mindset Mastering AWS DevOps Services: Focusing on CI/CD Pipelines Infrastructure as Code (IaC): Concepts and Tools Container Services on AWS Monitoring \u0026amp; Observability DevOps Best Practices \u0026amp; Case Studies Speakers Truong Quang Tinh Nghiem Le Long Huynh Quy Pham Key Highlights DevOps Mindset Core Values: Collaboration, shared responsibility, and breaking down silos. Automation: The goal is to \u0026ldquo;automate everything.\u0026rdquo; Culture: Continuous learning, experimentation, and data-driven measurement. The DevOps Journey Guide Do: Start with fundamentals. Learn by building real-world projects. Document everything you do. Master one concept/tool at a time. Enhance soft skills (communication/collaboration). Don\u0026rsquo;t: Stay in \u0026ldquo;tutorial hell\u0026rdquo; (learning without doing). Copy-paste code blindly without understanding. Compare your progress to others. Give up after initial failures. CI/CD Pipeline Lifecycle: Understanding the full application lifecycle from coding, testing, and code review, to pre-prod and final production environments. Infrastructure as Code (IaC) Concept: Managing cloud resources via code rather than manual GUI clicks. Benefits: Automatically create, update, and delete infrastructure with consistency. Tools: Terraform, OpenTofu, Pulumi. Container Services on AWS Ecosystem: Managing containers using Docker, Kubernetes, Amazon ECR, and Amazon EKS. Amazon App Runner: A fast, simple, and cost-effective service to deploy web applications directly from source code or container images without managing servers or configuring infrastructure. Monitoring \u0026amp; Observability Implementing best practices using Amazon CloudWatch and Amazon X-Ray to ensure system health and traceability. Key Takeaways DevOps Metrics Purpose: To monitor deployment health, improve agility, ensure system stability, optimize customer experience, and justify technology investments. The \u0026ldquo;Continuous\u0026rdquo; in CI/CD Gained a comprehensive view of the full CI/CD pipeline process (Continuous Integration, Continuous Delivery, Continuous Deployment). IaC in AWS Utilizing Amazon CloudFormation to create templates with defined services and parameters for repeatable deployments. Applying to Work Career Path: Develop a structured roadmap for a DevOps career. Implementation: Apply CI/CD pipelines to current active projects. Optimization: Build reusable IaC templates to reduce human error in infrastructure management. Event Experience Attending the “DevOps on AWS” workshop was extremely valuable. It provided me with a clear career guide, essential tools, and foundational knowledge. Key experiences included:\nInsights from Highly Skilled Speakers FACJ experts shared their personal experiences and daily realities of DevOps roles. Witnessed live demos of monitoring and observability in real-world projects. Exploring CI/CD and IaC Learned how large enterprises update their production environments continuously through robust CI/CD pipelines. Understood how to use constructs, reusable patterns, and language support (via AWS CDK). Practical exposure to deploying resources with CloudFormation. Monitoring and Observability Learned how to set up effective alerting, dashboards, and on-call processes. Event Photos Overall: The event provided a comprehensive view of the DevOps career path. Furthermore, it offered best practices on how to effectively apply CI/CD, monitoring, and observability in real projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Create IAM User for this workshop In AWS Management Console, search and choose IAM Navigate to User, click Create user For User name, enter admin-user Check Provide user access to the AWS Management Console - optional For Console password, check Custom password Enter password for your user Uncheck Users must create a new password at next sign-in - Recommended for easy operation Click Next For Permissions options, check Attach policies directly Click Create policy You will be directed to Create policy page For Policy editor, switch to JSON Copy and paste this policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;InfrastructureManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;iam:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;BackendComputeAndAPI\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;execute-api:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DatabaseAndAuth\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IoTServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iot:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;StorageAndHosting\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;MonitoringAndLogging\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;events:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next For Policy name, enter your policy name (E.g. SmartOfficeAdminFullAcccess) Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create policy Go back to Step 2 Set permissions of Create user Search and choose your policy name Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create user Login with your User account to begin this workhop "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Smart Office Management System for Lab Research A Unified AWS Serverless Solution for Real-Time Monitoring \u0026amp; Administration 1. Executive Summary The Smart Office Management System is a strategic initiative proposed by Team Skyscraper from FPTU HCM Campus. Inspired by the operational excellence observed during a field trip to the AWS office in Ho Chi Minh City, this project aims to modernize lab management. Traditional office oversight lacks real-time visibility into environmental conditions (temperature, humidity, light) and relies heavily on manual intervention. To address this, we propose a centralized Management Console built on a fully AWS Serverless architecture. By leveraging services such as AWS IoT Core, Lambda, and DynamoDB, the system collects sensor data at 2-5 minute intervals to support real-time monitoring and enable remote device configuration. This project also serves as a pivotal \u0026ldquo;First Cloud AI Journey,\u0026rdquo; enabling the team to bridge the gap between theoretical knowledge and the practical application of Cloud Computing.\n2. Problem Statement The Challenge Currently, managing research lab environments requires manual effort to check device statuses (lights, air conditioners) and environmental conditions. Managers often lack the granular data needed to make informed decisions regarding energy usage or occupant comfort. Operating devices on static schedules (e.g., 8:00 AM to 5:00 PM) without considering actual room usage or environmental factors leads to significant energy waste. Furthermore, the absence of a centralized dashboard makes it difficult for administrators to detect anomalies or configure settings across multiple rooms efficiently.\nThe Solution Our platform utilizes AWS IoT Core to ingest MQTT telemetry from room sensors, AWS Lambda and Amazon API Gateway for backend logic and processing, Amazon DynamoDB for storing sensor logs and room configurations, and Amazon S3 combined with Amazon CloudFront to host the web management dashboard. Security is paramount, with access strictly managed via Amazon Cognito. Amazon EventBridge orchestrates scheduled automation tasks, while Amazon SNS ensures timely notifications for system alerts. This solution transforms manual tracking into a digital, real-time management console capable of monitoring multiple rooms simultaneously.\nBenefits and Return on Investment (ROI) The Smart Office Management System enhances operational efficiency by providing a \u0026ldquo;single pane of glass\u0026rdquo; for monitoring and configuration. It empowers lab managers to control devices remotely and make data-driven decisions. Beyond operational improvements, the project establishes a reusable serverless foundation for future IoT research at the university.\nCost Efficiency: Monthly operating costs are estimated at just $1.81 USD, leveraging the AWS Free Tier for services like Lambda, API Gateway, and DynamoDB. The primary costs are attributed to CloudFront ($1.27) and CloudWatch ($0.25), totaling approximately $21.72 USD per year. Since the system utilizes existing ESP32 hardware (or simulation scripts during dev), there are no additional capital expenditures. The system delivers immediate value through time savings and reduced management overhead.\n3. Solution Architecture The Smart Office system adopts a fully serverless AWS architecture optimized for cost, performance, and scalability. Data from multiple sensor hubs is transmitted to AWS IoT Core, processed by Lambda functions, and stored in DynamoDB for real-time monitoring. EventBridge automates scheduled actions, while SNS handles system notifications. The web dashboard is hosted on S3 and delivered securely via CloudFront, with user authentication managed through Amazon Cognito. This architecture minimizes operational overhead and ensures high reliability.\nAWS Services Used AWS IoT Core: Ingests and manages MQTT data from smart room hubs, enabling secure communication between edge devices and the cloud. AWS Lambda: Serverless compute that executes backend logic for processing sensor telemetry, handling API requests, and executing management commands. Amazon API Gateway: Exposes secure RESTful endpoints for the web dashboard to interact with backend services. Amazon DynamoDB: A fast, NoSQL database for storing room configurations, device states, and historical sensor logs. Amazon EventBridge: Orchestrates event-driven workflows, such as scheduled configuration updates or heartbeat checks. Amazon SNS: Sends email notifications to administrators regarding system alerts or critical updates. Amazon S3: Hosts frontend static assets (HTML, CSS, JS) for the Management Dashboard. Amazon CloudFront: Delivers the web application globally with low latency and SSL security. Amazon Cognito: Manages user identity, authentication, and access control for the Management Console. Amazon CloudWatch: Collects logs and metrics to monitor system health and debug Lambda executions. Component Design Sensor Hubs: IoT-enabled devices (ESP32) in each room collect telemetry (temperature, humidity, light) and transmit it to AWS IoT Core periodically. Data Ingestion: AWS IoT Core rules trigger the HandleTelemetry Lambda, which validates data and persists it to Amazon DynamoDB. Configuration Management: Administrators use the dashboard to update room settings. The RoomConfigHandler Lambda updates DynamoDB and pushes changes to devices via IoT Core Shadows or MQTT topics. User Interaction: The Web Dashboard (served via S3/CloudFront) visualizes real-time data and provides a control interface. User Authentication: Amazon Cognito ensures only authorized lab members can log in and access sensitive room data. Monitoring \u0026amp; Reliability: Amazon CloudWatch tracks system performance, ensuring high availability and rapid troubleshooting. 4. Technical Implementation Implementation Phases\nPhase 1: Research \u0026amp; Foundation (Weeks 1-7): Deep dive into core AWS services (IoT Core, Lambda, DynamoDB, S3, API Gateway, Cognito) and study Serverless design patterns. Phase 2: Design \u0026amp; Estimation (Week 8): Finalize the solution diagram for an 8-room setup and use the AWS Pricing Calculator to forecast the budget. Phase 3: Development (Weeks 9-12): IoT: Implement firmware/scripts for IoT data simulation. Backend: Develop Lambda functions, DynamoDB tables, and API Gateway resources using CloudFormation/CDK. Frontend: Build the Management Dashboard and integrate it with APIs. Phase 4: Testing \u0026amp; Deployment (Week 13): Perform end-to-end testing, validate data flow from sensors to the dashboard, and deploy the system to the production environment. Technical Requirements\nHardware Layer: ESP32-based Sensor Hubs (or simulation scripts) monitoring environmental metrics. Cloud Layer: A fully serverless stack on AWS (IoT Core, Lambda, DynamoDB, API Gateway, S3, CloudFront, Cognito, EventBridge, SNS). DevOps: Infrastructure as Code (IaC) using AWS CloudFormation for reproducible deployments. Interface: A responsive web dashboard allowing real-time monitoring and configuration updates. 5. Timeline \u0026amp; Milestones Weeks 1–7: AWS Service mastery and completion of \u0026ldquo;First Cloud AI Journey\u0026rdquo; fundamental training. Week 8: System architecture design and cost estimation finalization. Weeks 9–12: Core development phase (Backend logic, Database schema, Frontend UI integration). Week 13: System integration testing, debugging, and Go-Live presentation. 6. Budget Estimation You can view the detailed budget estimation on the AWS Pricing Calculator or download the Budget Estimation File.\nOperational Costs (Monthly):\nAmazon DynamoDB: Free (Always Free Tier: 25GB Storage). AWS Lambda: Free (Always Free Tier: 1M requests/month). AWS IoT Core: ~$0.18 (8 devices, sending data every 2 mins). Amazon API Gateway: Free (Free Tier: 1M calls/month for 12 months). Amazon S3: Free (Standard storage \u0026lt; 5GB). Amazon CloudFront: ~$1.27 (Based on est. data transfer and HTTP requests). Amazon EventBridge: Free (Free Tier events). Amazon SNS: ~$0.02 (Email notifications). Amazon CloudWatch: ~$0.25 (Log ingestion and storage). Amazon Cognito: Free (Free Tier: 50,000 MAUs). Hardware Layer: $0.00 (Using existing hardware or Mock Scripts). Total: ≈ $1.81/month (approx. $21.72/year), highly optimized within the AWS Free Tier.\n7. Risk Assessment Risk Matrix IoT Connectivity Issues: Medium Impact, Medium Probability. Sensor Data Inaccuracy: Medium Impact, Low Probability. Unexpected AWS Charges: Low Impact, Low Probability (mitigated by budget alerts). Security Misconfiguration: High Impact, Low Probability. Mitigation Strategies Connectivity: Implement retry logic on edge devices and local data buffering. Cost Control: Configure AWS Budgets to alert when spending exceeds $5.00/month. Security: Enforce strict IAM policies (Least Privilege) and require authentication for all API access via Cognito. Reliability: Use CloudWatch Logs to trace errors in Lambda execution immediately. Contingency Plans Enable manual override controls if the cloud system becomes unavailable. Maintain a backup of CloudFormation templates for rapid disaster recovery/redeployment. 8. Expected Outcomes Technical Improvements: Transition from manual checks to real-time digital monitoring. Establish a centralized platform for managing configurations across multiple rooms. Create a scalable architecture capable of supporting more devices/sensors in the future. Long-term Value: Serves as a practical learning hub for students to master AWS Serverless technologies. Provides actionable data insights that can lead to better energy usage policies. Demonstrates a cost-effective, production-ready cloud solution suitable for university deployment. Proposal Link Smart_Office_Proposal\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Open Protocols for Agent Interoperability Part 3: Strands Agents &amp; MCP","tags":[],"description":"","content":"Open Protocols for Agent Interoperability Part 3: Strands Agents \u0026amp; MCP By Nick Aldridge, James Ward, and Clare Liguori | 10 JUL 2025\nCategories: Artificial Intelligence, Customer Solutions, Open Source\nIntroduction Developers are architecting and building systems of AI agents that work together to autonomously accomplish users\u0026rsquo; tasks. In Part 1 of our blog series on Open Protocols for Agent Interoperability we covered how Model Context Protocol (MCP) can be used to facilitate inter-agent communication and the MCP specification enhancements AWS is working on to enable that. The examples in Part 1 used Spring AI and Java for building the agents and connecting them with MCP. In Part 2 we discussed Authentication on MCP. This is a critical aspect of connecting agents so they can work together in a larger system, all with knowledge of who the user is.\nIn Part 3 we will show how you can build inter-agent systems with the new Strands Agents SDK and MCP.\nStrands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of Python code. You can read more about Strands Agents in the documentation. Since Strands Agents supports MCP we can quickly build a system consisting of multiple inter-connected agents and then deploy the agents on AWS.\nOur example is an HR agent which can answer questions about employees. To do this you could imagine the HR agent communicating with a number of other agents like an employee data agent, an Enterprise Resource Planning (ERP) agent, a performance agent, goal agent, etc. For this example let\u0026rsquo;s start with a basic architecture where a REST API exposes access to an HR agent which connects to an employee info agent:\nHR Agent Example Architecture\nNote: The complete, runnable version of the following example is available in our Agentic AI samples repo.\nCreate a System of Agents with Strands Agents and MCP Let\u0026rsquo;s start with the MCP server that will expose the employee data for use in the employee info agent. This is a basic MCP server:\nfrom mcp.server.fastmcp import FastMCP mcp = FastMCP(\u0026#34;employee-server\u0026#34;, stateless_http=True, host=\u0026#34;0.0.0.0\u0026#34;, port=8002) @mcp.tool() def get_skills() -\u0026gt; set[str]: \u0026#34;\u0026#34;\u0026#34;all of the skills that employees may have - use this list to figure out related skills\u0026#34;\u0026#34;\u0026#34; return SKILLS @mcp.tool() def get_employees_with_skill(skill: str) -\u0026gt; list[dict]: \u0026#34;\u0026#34;\u0026#34;employees that have a specified skill - output includes fullname (First Last) and their skills\u0026#34;\u0026#34;\u0026#34; skill_lower = skill.lower() return [employee for employee in EMPLOYEES if any(s.lower() == skill_lower for s in employee[\u0026#34;skills\u0026#34;])] if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#34;streamable-http\u0026#34;) This MCP server exposes the ability to get a list of employee skills and get a list of employees that have a specific skill. For this example we are using fictitious data and in a future blog we will share how to secure this service.\nNow that we have a way for an agent to get this employee information, let\u0026rsquo;s create the employee agent using Strands Agents. Our agent connects to the MCP server for employee information and uses Bedrock for inference. We also add a system prompt to provide some additional behavior to this agent:\nEMPLOYEE_INFO_URL = os.environ.get(\u0026#34;EMPLOYEE_INFO_URL\u0026#34;, \u0026#34;http://localhost:8002/mcp/\u0026#34;) employee_mcp_client = MCPClient(lambda: streamablehttp_client(EMPLOYEE_INFO_URL)) bedrock_model = BedrockModel( model_id=\u0026#34;amazon.nova-micro-v1:0\u0026#34;, region_name=\u0026#34;us-east-1\u0026#34;, ) def employee_agent(question: str): with employee_mcp_client: tools = employee_mcp_client.list_tools_sync() agent = Agent(model=bedrock_model, tools=tools, system_prompt=\u0026#34;you must abbreviate employee first names and list all their skills\u0026#34;, callback_handler=None) return agent(question) This example uses Amazon Bedrock and the Nova Micro model with the employee data tool for multi-turn inference. Multi-turn inference is when an AI agent makes multiple calls to an AI model in a loop, usually involving calling tools to get or update data, completing when the initial task is done or an error happens. In this example the multi-turn inference enables a flow like:\nUser asks \u0026ldquo;list employees who have skills related to AI\u0026rdquo; The LLM sees that it has access to a list of employee skills and instructs the agent to call that tool The agent calls the employee get_skills tool and returns the skills to the LLM The LLM then determines which skills are related to AI and sees that it can then get employees with each skill using the get_employees_with_skill tool The agent gets the employees for each skill and returns them to the LLM The LLM then assembles the complete list of employees with skills related to AI and returns it This multi-turn interaction across multiple LLM calls and multiple tool calls encapsulated into a single agent(question) call, shows the power of Strands Agents performing an agentic loop to achieve a provided task. This example also shows how the employee agent can add additional instructions on top of the underlying tools, in this case with a system prompt.\nExpose an Agent as an MCP Server We can interact with this agent in a number of ways. For example, we could expose it as a REST service. In our case we want to expose it in a way that will enable other agents to interact with it. We can use MCP to facilitate that inter-agent communication by exposing this agent as an MCP server. Then another agent (for example HR agent) will be able to use the employee agent as a tool.\nTo expose the employee agent as an MCP server we just wrap our employee_agent function in an @mcp.tool, transform the response data into a list of strings, and start the server:\nmcp = FastMCP(\u0026#34;employee-agent\u0026#34;, stateless_http=True, host=\u0026#34;0.0.0.0\u0026#34;, port=8001) @mcp.tool() def inquire(question: str) -\u0026gt; list[str]: \u0026#34;\u0026#34;\u0026#34;answers questions related to our employees\u0026#34;\u0026#34;\u0026#34; return [ content[\u0026#34;text\u0026#34;] for content in employee_agent(question).message[\u0026#34;content\u0026#34;] if \u0026#34;text\u0026#34; in content ] if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#34;streamable-http\u0026#34;) Since we\u0026rsquo;ve wrapped the employee agent in an MCP server we can now use it in other agents. The HR agent is exposed as a REST API so that we could call it from a web application or other services.\nEMPLOYEE_AGENT_URL = os.environ.get(\u0026#34;EMPLOYEE_AGENT_URL\u0026#34;, \u0026#34;http://localhost:8001/mcp/\u0026#34;) hr_mcp_client = MCPClient(lambda: streamablehttp_client(EMPLOYEE_AGENT_URL)) bedrock_model = BedrockModel( model_id=\u0026#34;amazon.nova-micro-v1:0\u0026#34;, region_name=\u0026#34;us-east-1\u0026#34;, temperature=0.9, ) app = FastAPI(title=\u0026#34;HR Agent API\u0026#34;) class QuestionRequest(BaseModel): question: str @app.get(\u0026#34;/health\u0026#34;) def health_check(): return {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;} @app.post(\u0026#34;/inquire\u0026#34;) async def ask_agent(request: QuestionRequest): async def generate(): with hr_mcp_client: tools = hr_mcp_client.list_tools_sync() agent = Agent(model=bedrock_model, tools=tools, callback_handler=None) async for event in agent.stream_async(request.question): if \u0026#34;data\u0026#34; in event: yield event[\u0026#34;data\u0026#34;] return StreamingResponse( generate(), media_type=\u0026#34;text/plain\u0026#34; ) if __name__ == \u0026#34;__main__\u0026#34;: uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) Run MCP and Strands Agents on AWS Of course we can run all of this on AWS using a variety of different options. Since the MCP servers use the new MCP Streamable HTTP transport, this could be run on serverless runtimes like AWS Lambda or AWS Fargate. For this example we will containerize the employee info MCP server, the employee agent, and the HR agent, run them on ECS, and expose the HR agent through a load balancer:\nMCP and Strands Agents on AWS Infrastructure\nFor this example we\u0026rsquo;ve used AWS CloudFormation to define the infrastructure (source). Now with everything running on AWS we can make a request to the HR agent:\ncurl -X POST --location \u0026#34;http://something.us-east-1.elb.amazonaws.com/inquire\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;question\u0026#34;: \u0026#34;list employees that have skills related to AI programming\u0026#34;}\u0026#39; And we get back:\nHere are the employees with skills related to AI programming: 1. W. Rodriguez - Machine Learning, REST API 2. M. Rodriguez - DevOps, Machine Learning, Python 3. R. Rodriguez - Machine Learning, JavaScript 4. J. Rodriguez - REST API, Kubernetes, Machine Learning, Node.js 5. W. Garcia - AWS, Kubernetes, GraphQL, Machine Learning 6. W. Davis - MongoDB, Angular, Kotlin, Machine Learning, REST API 7. J. Miller - React, Machine Learning, SQL, Kotlin 8. J. Rodriguez - SQL, Machine Learning, Docker, DevOps, Git If you need more detailed information about any of these employees or require further assistance, please let me know! Get the complete source for this example.\nMCP Contributions from AWS This example shows just the beginning of what we can do with Strands Agents and MCP for inter-agent communication. We\u0026rsquo;ve been working with the MCP specification and implementations to help evolve MCP to support additional capabilities that some inter-agent use cases may need.\nThe MCP specification has just had a new 2025-06-18 release which includes two contributions from AWS to better support inter-agent communication. We\u0026rsquo;ve also contributed support for these new features in various MCP implementations.\n1. Elicitation Elicitation: When MCP servers need additional input they can signal that to the agent (via the MCP client). Instead of a tool call providing a data response it can elicit additional information. For example, if an Employee Info Agent using an MCP tool to get employee data determines that a tool request like \u0026ldquo;get employees that have AI skills\u0026rdquo; may return more results than the user may want, it can elicit that the user provide a team name to filter on. This approach is different from a tool parameter because runtime characteristics may determine that the team name is necessary.\nIn a future blog post in this series we will dive deeper into how to use this feature. We\u0026rsquo;ve contributed the implementations of this specification change to the Java and Python MCP SDKs, with both having merged the changes:\nJava SDK change Python SDK change 2. Structured Output Schemas Structured Output Schemas: MCP tools generally return data to the agent (via the MCP client) but in contrast with input schemas (which have always been required), we\u0026rsquo;ve contributed a way for tools to optionally specify an output schema. This enables agents to perform more type-safe conversions of tool outputs to typed data, enabling safer and easier transformations of that data within the agent.\nWe will also cover this feature more in future blog posts in this series. Implementations of this feature have been contributed to MCP\u0026rsquo;s Java, Python, and TypeScript SDKs with the TypeScript implementation having already been merged:\nJava SDK pull request Python SDK pull request TypeScript SDK pull request Conclusion It\u0026rsquo;s exciting to see the progress with MCP for inter-agent communication and in future parts of this series we will dive deeper into how these enhancements can be used to enable richer interactions between agents.\nIn this blog we\u0026rsquo;ve seen how we can use Strands Agents and MCP to create a system of agents that all work together and run on AWS. Strands Agents made it easy to build an agent connected to MCP tools, and then also expose the agent as an MCP server so that other agents can communicate with it. To get started learning about Strands Agents, check out the documentation and GitHub repo. Stay tuned for more parts of this series on Agent Interoperability!\nAbout the Authors Nick Aldridge Nick Aldridge is a Principal Engineer at AWS. Over the last 6 years, Nick has worked on multiple AI/ML initiatives including Amazon Lex and Amazon Bedrock. Most recently, he led the team that launched Amazon Bedrock Knowledge Bases. Today he works on generative AI and AI infrastructure with a focus on inter-agent collaboration and function calling. Prior to AWS, Nick earned his MS at the University of Chicago.\nJames Ward James Ward is a Principal Developer Advocate at AWS. James travels the world helping enterprise developers learn how to build reliable systems. His current focus is on helping developers build systems of AI agents using Spring AI, Embabel, Strands Agents, Amazon Bedrock, MCP, and A2A.\nClare Liguori Clare Liguori is a Senior Principal Software Engineer for AWS Agentic AI. She focuses on re-imagining how applications are built and how productive developers can be when their tools are powered by generative AI and AI agents, as part of Amazon Q Developer.\nResources Open Source at AWS Projects on GitHub Strands Agents Documentation Strands Agents GitHub Repository Model Context Protocol (MCP) AWS Samples - Agentic AI Demos Tags: Artificial Intelligence, Open Source, Model Context Protocol, Strands Agents, Amazon Bedrock, Multi-Agent Systems\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section contains key AWS blog posts and technical articles related to cloud infrastructure, data management, and distributed systems.\nBlog 1 - University of California Irvine Backs Up Petabytes of Research Data to AWS This blog presents a comprehensive case study on how UCI\u0026rsquo;s Research Cyber Infrastructure Center built a scalable backup solution for 10 PB of research data across 1500 faculty labs. Learn about the architecture using rclone, S3 Glacier Deep Archive, CloudWatch monitoring, and cost optimization strategies for managing massive research datasets on AWS.\nBlog 2 - Guide to IP and Domain Warming and Migrating to Amazon SES This guide provides best practices for successfully warming up IP addresses and domains when migrating email workloads to Amazon SES. Discover strategies for managing deliverability challenges, implementing a 40-day warm-up schedule, and maintaining sender reputation with mailbox providers like Gmail, Outlook, and Yahoo.\nBlog 3 - Open Protocols for Agent Interoperability Part 3: Strands Agents \u0026amp; MCP This technical article explores building multi-agent systems using Strands Agents framework combined with Model Context Protocol (MCP). Learn how to create specialized agents for HR automation, implement agent communication patterns, and deploy scalable AI systems on AWS infrastructure.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog: Architecting VPC &amp; Secure Compute","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 3 Objectives: Implement Cloud Networking: Move beyond default settings to design a custom Amazon VPC (Virtual Private Cloud). Secure Compute Identity: Replace long-term credentials (access keys) with temporary credentials using IAM Roles for EC2. Deploy Scalable Compute: Master the lifecycle of Amazon EC2 instances within a custom network. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study VPC Fundamentals: + CIDR blocks \u0026amp; Subnetting (Public vs. Private). + Internet Gateways (IGW) \u0026amp; Route Tables. + Lab: \u0026ldquo;Deploy Network Infrastructure with Amazon VPC\u0026rdquo;. 08/18/2025 08/18/2025 FCJ: Deploy Network Infrastructure (VPC) 3 - Study EC2 \u0026amp; IAM Roles: + Understanding the separation of Compute (EC2) and Identity (IAM). + Lab: \u0026ldquo;Grant permissions to applications via IAM Role\u0026rdquo;. + Goal: Allow EC2 to access S3 without hardcoded AWS keys. 08/19/2025 08/19/2025 FCJ: IAM Roles for EC2 4 - Practice: Custom VPC Build: + Create a VPC 10.0.0.0/16. + Create Public Subnet 10.0.1.0/24. + Configure Route Table to point 0.0.0.0/0 to IGW. 08/20/2025 08/20/2025 AWS VPC User Guide 5 - Practice: Secure Web Server Deployment: + Launch EC2 into the Custom VPC. + Attach an IAM Role (S3ReadOnlyAccess). + Use User Data script to install Apache (httpd) at launch. 08/21/2025 08/22/2025 FCJ: Launch App on EC2 6 - Bonus/Review: + Static Website Hosting: Review S3 basics by hosting a simple static page. + Verify EC2 connectivity and IAM role access via CLI inside the instance. 08/22/2025 08/22/2025 FCJ: S3 Static Website Week 3 Achievements: Architected a Custom Network Topology (VPC):\nSuccessfully completed the First Cloud Journey VPC Module. Built a custom Virtual Private Cloud (VPC) with defined CIDR blocks, confirming understanding of network isolation. Configured Public Subnets and Internet Gateways, ensuring that only specific resources have exposure to the public internet. Implemented \u0026ldquo;Zero Trust\u0026rdquo; Principles with IAM Roles:\nMoved away from storing AWS Access Keys (AK/SK) on servers. Attached an IAM Role to an EC2 instance, proving the ability to grant temporary, rotating credentials to applications securely. Verification: Verified via the command aws sts get-caller-identity inside the EC2 instance that it assumed the correct role. Deployed Compute with Automation:\nLaunched an Amazon Linux 2 instance into the custom VPC. Used Bootstrapping (User Data) to auto-install a web server, adhering to the First Cloud Journey best practices for reproducible infrastructure. Explored S3 Capabilities:\n(Optional) Configured an S3 bucket for Static Website Hosting, understanding the difference between Block Storage (EBS) and Object Storage (S3). "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Security Foundations Identity \u0026amp; Access Management (IAM) Detection \u0026amp; Monitoring Infrastructure Protection Data Protection Incident Response Speakers Le Vu Xuan An Tran Duc Anh Tran Doan Cong Ly Danh Hoang Hieu Nghi Thinh Lam Viet Nguyen Mendel Branski (Long) Key Highlights Introduction to Cloud Clubs Introduction to University Cloud Clubs (e.g., UTE, SGU). Overview of Cloud Club activities and community building. Security Foundation Service Control Policies (SCPs). Permission Boundaries. Multi-Factor Authentication (MFA). Detection and Monitoring Multi-Layer Security Visibility. Alerting \u0026amp; Automation with Amazon EventBridge. Detection-as-Code. Network and Data Protection VPC Security Group Sharing. API-Based Services Security. Secrets Management. Incident Response Prevention: Why nobody has time for incidents. Mental Health: A guide to \u0026ldquo;sleeping better\u0026rdquo; by reducing alert fatigue. Process: structuring the incident response workflow. Key Takeaways Service Control Policies (SCPs) Definition: An organizational policy type. Function: Controls the maximum available permissions for all accounts within the organization. Rule: SCPs never grant permissions; they can only filter or restrict them. Permission Boundaries Purpose: An advanced IAM feature designed to solve delegation issues. Function: Sets the maximum permissions that an identity-based policy can grant to a specific User or Role. Multi-Factor Authentication (MFA) TOTP: Shared secret based, requires a manual 6-digit code (e.g., Google Authenticator). It is free and offers flexible backup/recovery. FIDO2: Uses public-key cryptography, requires a simple touch/biometric scan (e.g., YubiKey). It is highly secure but requires strict backup strategies. Alerting \u0026amp; Automation with EventBridge Real-time Events: CloudTrail events flow to EventBridge for immediate processing. Automated Alerting: Detect suspicious activities across all organization accounts. Cross-account Event Routing: Centralized event processing and automated response. Integration \u0026amp; Workflows: Integration with SNS, Slack, and SQS for automated security responses and team notifications. Detection-as-Code IaC Deployment: Deploy Amazon GuardDuty across the organization using CloudFormation/Terraform (enable protection plans, configure data sources). Custom Detection Rules: Build suppression rules \u0026amp; IP whitelists to reduce false positives and adapt to the specific environment. Version-Controlled Logic: Track detection rules in Git and integrate them into the DevSecOps pipeline for testing and deployment. Incident Response Preparation: Prepare automation handlers for potential incidents. Forecasting: Predict future incident scenarios and design response plans. Post-Incident: Document \u0026ldquo;Lessons Learned\u0026rdquo; after each incident to prevent recurrence. Applying to Work Least Privilege: Strictly specify and enforce least privilege policies for all projects. MFA Enforcement: Apply MFA to every account (Root and IAM users). Proactive Planning: Predict potential failure points and prepare response plans. Event Experience Attending the “AWS Well-Architected Security Pillar” workshop helped me improve my knowledge of security and incident response significantly. I gained insights into the AWS Security Pillars through:\nLearning from Skilled Speakers Learned how Senior Engineers handle high-pressure incidents and the \u0026ldquo;post-mortem\u0026rdquo; process. Learned how to protect data and networks using native AWS security features. Exploring Cloud Club Activities Connected with the AWS Learner community from various universities. Alerting \u0026amp; Automation Gained the ability to prepare infrastructure using CloudTrail, EventBridge, and CloudWatch to manage resources in real-time as soon as an incident occurs. Overall: The event was a great chance for me to expand my knowledge in Alerting, Automation, Security, and Incident Response. I have gained a lot of experience by listening to Senior Cloud Engineers discuss their daily work.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-run-cloudformation-stack/","title":"Set up cloudformation","tags":[],"description":"","content":"Download resources Download these CloudFormation template files:\nsmart_office_budget.yaml smart_office_s3_cloudfront.yaml smart_office_cognito.yaml smart_office_dynamodb.yaml smart_office_lambda_authenticate_with_dynamodb_cognito.yaml smart_office_lambda_readonly_with_dynamodb.yaml smart_office_lambda_crud_with_dynamodb_cognito.yaml smart_office_lambda_crud_with_dynamodb_iot.yaml smart_office_iot_core.yaml smart_office_api_gateway.yaml Deploy CloudFormation Stacks In AWS Management Console, search and choose CloudFormation Click Create stack For Prepare template, check Choose an existing template For Template source, check Upload a template file Click Choose file Choose file smart_office_budget.yaml Click Next For Stack name, enter SmartOffice-Budget-Dev Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) For Stack failure options, check Preserve successfully provisioned resources (To keep created resource for debugging) Click Next Check again and click Submit Do the same for other files with exactly name Template name Stack name smart_office_s3_cloudfront.yaml SmartOffice-S3-CloudFront-Dev smart_office_cognito.yaml SmartOffice-Cognito-Dev smart_office_dynamodb.yaml SmartOffice-DynamoDB-Dev smart_office_lambda_authenticate_with_dynamodb_cognito.yaml SmartOffice-Authenticate-Lambda-Dev smart_office_lambda_readonly_with_dynamodb.yaml SmartOffice-ReadOnly-Lambda-Dev smart_office_lambda_crud_with_dynamodb_cognito.yaml SmartOffice-Crud-Lambda-Dev smart_office_lambda_crud_with_dynamodb_iot.yaml SmartOffice-IoT-Lambda-Dev smart_office_iot_core.yaml SmartOffice-IoT-Core-Dev smart_office_api_gateway.yaml SmartOffice-API-Gateway-Dev "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog: Managed Databases &amp; Storage","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 4 Objectives: Master Managed Services: Understand the operational benefits of using Amazon RDS versus installing a database manually on EC2. Implement a 2-Tier Architecture: Connect a web server (EC2) securely to a database instance (RDS) within the VPC. Host Static Content: Deploy a serverless frontend using Amazon S3 Static Website Hosting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Database Fundamentals on Cloud: + Managed vs. Unmanaged (Why use RDS?). + DB Subnet Groups: Placing databases in Private Subnets. + Security: Encryption at rest and Security Group referencing. 08/25/2025 08/25/2025 FCJ: Create Database on RDS 3 - Practice: Launch RDS MySQL: + Create a DB Subnet Group covering two private subnets. + Provision an RDS MySQL instance (Free Tier). + Critical: Configure Master User credentials securely. 08/26/2025 08/26/2025 Creating an Amazon RDS DB instance 4 - Practice: Security Group Chaining: + Create a DB-SG Security Group. + DevOps Task: Allow Inbound on Port 3306 only from the WebServer-SG ID (Source: sg-xxxxx), NOT from 0.0.0.0/0. 08/27/2025 08/27/2025 Controlling Access with Security Groups 5 - Practice: Connect App to DB: + SSH into the EC2 instance (from Week 3). + Install mysql-client. + Verify connectivity: mysql -h \u0026lt;RDS-Endpoint\u0026gt; -u admin -p. 08/28/2025 08/28/2025 Connecting to an RDS instance 6 - Static Website Project: + Create an S3 Bucket with public read access (Block Public Access settings). + Enable \u0026ldquo;Static Website Hosting\u0026rdquo;. + Upload index.html and error.html. 08/29/2025 08/29/2025 FCJ: Hosting Static Website with S3 Week 4 Achievements: Deployed a Managed Database (RDS):\nSuccessfully provisioned a MySQL RDS instance within the VPC. Understood the \u0026ldquo;Shared Responsibility Model\u0026rdquo;: AWS manages the OS patching and backups, while I manage the data and schema. Implemented Secure Network Segmentation (2-Tier Arch):\nPlaced the Database in Private Subnets, ensuring it is not directly accessible from the internet. Used Security Group Chaining (Referencing): Configured the Database Security Group to strictly allow traffic only from the Web Server\u0026rsquo;s Security Group ID. This is a critical DevSecOps pattern that avoids hardcoding IP addresses. Verified Application-Tier Connectivity:\nInstalled the MySQL client on the EC2 Jumpbox/Web Server. Successfully established a connection to the RDS Endpoint, validating the route tables and firewall rules. Deployed Serverless Frontend (S3):\nConfigured an Amazon S3 bucket for static website hosting. managed Bucket Policies to allow public read access specifically for web assets, distinguishing between \u0026ldquo;Private Storage\u0026rdquo; and \u0026ldquo;Public Hosting\u0026rdquo; use cases. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee Description: Introduction to Foundation Models and Generative AI capabilities using Amazon Bedrock. The talk focused on effective prompting techniques (Chain of Thought) and the architecture of Retrieval-Augmented Generation (RAG) for integrating internal knowledge bases. Detailed exploration of vector embeddings with Amazon Titan and an overview of various AWS AI services.\nOutcome: Gained comprehensive knowledge on reducing AI hallucinations and enhancing response accuracy through RAG and Embeddings. Learned how to apply specific AWS AI services and prompting strategies to real-world projects. The event provided valuable insights into building AI agents and offered opportunities to network with skilled speakers and industry experts.\nEvent 2 Event Name: AWS Cloud Mastery Series #2: ​DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025 Location: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City Role: Attendee Description: Comprehensive overview of the DevOps mindset, emphasizing collaboration, automation, and shared responsibility. The session covered essential AWS DevOps services including CI/CD pipelines, Infrastructure as Code (IaC) with CloudFormation and Terraform, and container management using Amazon EKS and AppRunner. It also provided a guide on the \u0026ldquo;Do\u0026rsquo;s and Don\u0026rsquo;ts\u0026rdquo; of the DevOps journey and best practices for monitoring and observability.\nOutcome: Gained a clear roadmap for a DevOps career and a deeper understanding of the application life cycle. Learned to implement CI/CD pipelines and utilize IaC templates to minimize human errors. Acquired insights into monitoring deployment health and system stability through real-world case studies and expert demonstrations.\nEvent 3 Event Name: AWS Cloud Mastery Series #3: ​Theo AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee Description: Dive into the AWS Well-Architected Security Pillar, which include Identity \u0026amp; Access Management, Infrastructure Protection, and Data Protection. The session explored foundational security measures like Service Control Policies (SCPs), Permission Boundaries, and MFA. It also highlighted advanced strategies for \u0026ldquo;Detection-as-Code\u0026rdquo; using IaC tools and automated incident response workflows with Amazon EventBridge.\nOutcome: Learned how to enforce least privilege principles effectively using SCPs and IAM boundaries. Gained practical skills in setting up automated alerting and cross-account event routing for real-time threat detection. Acquired valuable insights into incident response processes, emphasizing prevention and automation to minimize downtime and security risks.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-set-up-website/","title":"Set up website","tags":[],"description":"","content":"Set up Gitlab repository to deploy website and lambda code Go to this Gitlab repository: https://gitlab.com/tranngockhiet22062005/smart-office Download and deploy on your own repository Set up role for Gitlab to deploy website to S3 and deploy code to Lambda Function Create an IAM User with following attribute (view 5.2 if you forget) User name: smart-office-gitlab-ci Policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ListBucketAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev\u0026#34;, \u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadWriteAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev/*\u0026#34;, \u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaUpdateAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:*:*:function:SmartOffice-*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontInvalidation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } You should replace ACCOUNT_ID with your AWS Account ID\nPolicy name: SmartOfficeGitlabAccess Navigate to the user In Summary, click Create access key For Use case, check Command Line Interface (CLI) Check I understand the above recommendation and want to proceed to create an access key. Click Next Click Create access key Click Download .csv file Config Gitlab variables Go to your Gitlab repository, click Setting \u0026gt; Variables Click Add variable For each Key and Value follow the table bellow to know where to get value Key Value AWS_ACCESS_KEY_ID In your smart-office-gitlab-ci_accessKeys.csv you have downloaded AWS_DEFAULT_REGION ap-southeast-1 (or anywhere you deploy the workshop) AWS_SECRET_ACCESS_KEY In your smart-office-gitlab-ci_accessKeys.csv you have downloaded CLOUDFRONT_DISTRIBUTION_ID CloudFront \u0026gt; Distributions \u0026gt; Your distribution ID S3_BUCKET_FRONTEND fcj-smart-office-frontend-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) S3_BUCKET_LAMBDA fcj-smart-office-lambda-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) STACK_NAME_AUTH SmartOffice-Authenticate-Lambda-Dev STACK_NAME_CRUD SmartOffice-Crud-Lambda-Dev STACK_NAME_IOT SmartOffice-IoT-Lambda-Dev STACK_NAME_READONLY SmartOffice-ReadOnly-Lambda-Dev VITE_API_BASE_URL API Gateway \u0026gt; SmartOffice-API-Gateway-Dev-Api \u0026gt; Stages \u0026gt; Invoke URL Push code into an init branch Merge branchs in this order: init -\u0026gt; dev, dev -\u0026gt; main "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog: High Availability &amp; Auto Scaling","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 5 Objectives: Design for Failure: Eliminate Single Points of Failure (SPOF) by distributing traffic across multiple Availability Zones (AZs). Implement Elasticity: Configure Auto Scaling Groups (ASG) to automatically scale compute capacity out and in based on demand. Traffic Management: Deploy an Application Load Balancer (ALB) to act as the single entry point for the application. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study High Availability Concepts: + Vertical vs. Horizontal Scaling: Why scale out (add servers) vs. scale up (bigger CPU). + Load Balancing: Layer 7 (ALB) features like path-based routing. + Target Groups \u0026amp; Health Checks: How ALB detects bad instances. 09/01/2025 09/01/2025 Elastic Load Balancing (ELB) Overview 3 - Practice: Create Launch Template: + Create a Launch Template (modern replacement for Launch Config). + Define the AMI (Web Server from Week 3), Instance Type, Security Groups, and IAM Role. + DevOps Note: Ensure User Data is included for automated startup. 09/02/2025 09/02/2025 Creating a Launch Template 4 - Practice: Deploy Application Load Balancer (ALB): + Create an ALB in Public Subnets (spanning 2 AZs). + Create a Target Group for HTTP traffic (Port 80). + Configure Health Checks to ping /index.html. 09/03/2025 09/03/2025 Create an Application Load Balancer 5 - Practice: Configure Auto Scaling Group (ASG): + Create an ASG using the Launch Template. + Define capacity: Min: 2, Desired: 2, Max: 4. + Attach the ASG to the ALB Target Group. + Stress Test: Simulate CPU load to trigger a scale-out event. 09/04/2025 09/05/2025 Amazon EC2 Auto Scaling User Guide 6 - Verify Resilience: + Manually terminate one EC2 instance to test Self-Healing. + Observe ASG automatically launching a replacement. 09/05/2025 09/05/2025 Self-Verification Week 5 Achievements: Architected High Availability (HA):\nDeployed the application across multiple Availability Zones (AZs). Ensured that if one Data Center (AZ) goes offline, the application remains accessible via the Load Balancer. Implemented Immutable Infrastructure Patterns:\nTransitioned from manually launching instances to using Launch Templates. Standardized the server configuration (AMI, Security Groups, IAM), ensuring every new server is an exact replica of the \u0026ldquo;Gold Image.\u0026rdquo; Mastered Traffic Orchestration:\nConfigured an Application Load Balancer (ALB) to intelligently route internet traffic to healthy instances only. Implemented Health Checks, preventing the ALB from sending requests to failed or booting instances. Achieved Operational Resilience (Self-Healing):\nSuccessfully configured an Auto Scaling Group (ASG). Validated the \u0026ldquo;Self-Healing\u0026rdquo; capability: When an instance was manually terminated, the ASG detected the health check failure and automatically provisioned a replacement without human intervention. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-event-bridge/","title":"EventBridge and Lambda Setup","tags":[],"description":"","content":"Overview This section will guide you through setting up Amazon EventBridge and Lambda to route and react to events happening in DynamoDB. For SNS setup (used to send alerts), please refer to section 5.6 - SNS Setup.\nCreate AutomationSetup (Lambda + rules) Create a Lambda function (AutomationSetup) whose job is to read automation configuration from DynamoDB and define two EventBridge rules: one to turn automation ON and another to turn it OFF. import boto3 import json import os from boto3.dynamodb.types import TypeDeserializer deserializer = TypeDeserializer() events_client = boto3.client(\u0026#39;events\u0026#39;) HANDLER_ARN = os.environ.get(\u0026#39;HANDLER_LAMBDA_ARN\u0026#39;) def ddb_deserialize(image): d = {} for key in image: d[key] = deserializer.deserialize(image[key]) return d def time_to_cron(time_str): try: hour, minute = map(int, time_str.split(\u0026#39;:\u0026#39;)) utc_hour = hour - 7 if utc_hour \u0026lt; 0: utc_hour += 24 return f\u0026#34;cron({minute} {utc_hour} * * ? *)\u0026#34; except: return None # --- UPDATE 1: Add office_id parameter to function --- def create_or_update_schedule(room_id, office_id, time_str, action): rule_name = f\u0026#34;Room_{room_id}_Auto_{action}\u0026#34; cron_expr = time_to_cron(time_str) if not cron_expr: return print(f\u0026#34;Updating Rule: {rule_name} with Input\u0026#34;) events_client.put_rule( Name=rule_name, ScheduleExpression=cron_expr, State=\u0026#39;ENABLED\u0026#39;, Description=f\u0026#39;Auto {action} for {room_id} in {office_id}\u0026#39; ) # --- UPDATE 2: Add officeId to Input JSON --- target_input = json.dumps({ \u0026#34;roomId\u0026#34;: room_id, \u0026#34;officeId\u0026#34;: office_id, # \u0026lt;--- IMPORTANT: Include officeId \u0026#34;command\u0026#34;: action.upper(), \u0026#34;source\u0026#34;: \u0026#34;Scheduled_Event\u0026#34; }) events_client.put_targets( Rule=rule_name, Targets=[{ \u0026#39;Id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;Arn\u0026#39;: HANDLER_ARN, \u0026#39;Input\u0026#39;: target_input }] ) def lambda_handler(event, context): print(\u0026#34;Raw Event:\u0026#34;, json.dumps(event)) if \u0026#39;Records\u0026#39; in event: for record in event[\u0026#39;Records\u0026#39;]: if record[\u0026#39;eventName\u0026#39;] in [\u0026#39;INSERT\u0026#39;, \u0026#39;MODIFY\u0026#39;]: raw_image = record[\u0026#39;dynamodb\u0026#39;][\u0026#39;NewImage\u0026#39;] data = ddb_deserialize(raw_image) room_id = data.get(\u0026#39;roomId\u0026#39;) # --- UPDATE 3: Get officeId from DynamoDB --- office_id = data.get(\u0026#39;officeId\u0026#39;) auto_control = data.get(\u0026#39;autoControl\u0026#39;) auto_on = data.get(\u0026#39;autoOnTime\u0026#39;) auto_off = data.get(\u0026#39;autoOffTime\u0026#39;) if auto_control == \u0026#34;ON\u0026#34; and room_id: # Pass office_id to schedule creation function if auto_on: create_or_update_schedule(room_id, office_id, auto_on, \u0026#39;ON\u0026#39;) if auto_off: create_or_update_schedule(room_id, office_id, auto_off, \u0026#39;OFF\u0026#39;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Processed\u0026#39;} Go to Configuration \u0026ndash;\u0026gt; Trigger to configure the Lambda triggers — this will be used to invoke the lambda whenever there is any new stream from DynamoDB Select Add Trigger, Select DynamoDB and choose the table that contains rooms\u0026rsquo;s config. In Configuration \u0026ndash;\u0026gt; Environment Variable, add this key-value pair (replace accordingly with your personal information) Select Configuration \u0026ndash;\u0026gt;Role name, and make sure you have those 3 policies: AutomationSetup_RuleExecuiton for creating rules in EventBridge AWSLambdaBasicExecutionRole for basic lambda execution permissions AWSLambdaDynamoDBExecutionRole for interacting with DynamoDB. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;events:DeleteRule\u0026#34;, \u0026#34;events:PutTargets\u0026#34;, \u0026#34;events:EnableRule\u0026#34;, \u0026#34;events:PutRule\u0026#34;, \u0026#34;events:DisableRule\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Only for AutomationSetup_RuleExecuiton, select Add permissions \u0026ndash;\u0026gt; Create inline policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:261899902491:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:261899902491:log-group:/aws/lambda/AutomationSetup:*\u0026#34; ] } ] } AWSLambdaBasicExecutionRole\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } AWSLambdaDynamoDBExecutionRole\nThese two rules correspond to the ON and OFF automation behaviours.\nCreate AutomationHandler (Lambda to forward events to AWS IoT Core) Create the AutomationHandler Lambda to receives events from EventBridge and forwards them to AWS IoT Core. import boto3 import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) iot_client = boto3.client(\u0026#39;iot-data\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Input from EventBridge: {\u0026#34;roomId\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;officeId\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;ON\u0026#34;, ...} \u0026#34;\u0026#34;\u0026#34; # Log entire event to verify payload logger.info(f\u0026#34;Executing Automation: {json.dumps(event)}\u0026#34;) # 1. Extract data from Event room_id = event.get(\u0026#39;roomId\u0026#39;) command = event.get(\u0026#39;command\u0026#39;) # ON / OFF office_id = event.get(\u0026#39;officeId\u0026#39;) # 2. Validate input data if not room_id or not command: logger.error(\u0026#34;Missing roomId or command\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Missing roomId or command\u0026#39;} if not office_id: logger.error(\u0026#34;Missing officeId\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Missing officeId\u0026#39;} # 3. Create Topic and Payload topic = f\u0026#34;office/{office_id}/room/{room_id}/config\u0026#34; payload = { \u0026#34;command\u0026#34;: \u0026#34;SET_STATE\u0026#34;, \u0026#34;value\u0026#34;: command, \u0026#34;triggeredBy\u0026#34;: \u0026#34;Schedule\u0026#34; } # 4. Send command to IoT Core try: # This line must be aligned with lines above response = iot_client.publish( topic=topic, qos=1, payload=json.dumps(payload) ) logger.info(f\u0026#34;SUCCESS: Sent {command} to {topic}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Command sent\u0026#39;} except Exception as e: logger.error(f\u0026#34;IoT Publish Error: {e}\u0026#34;) # Raise error for EventBridge to know it failed (trigger Retry/DLQ) raise e Go to Configuration \u0026ndash;\u0026gt; Permissions and add resource-based policy. (To allow EventBridge to access this lambda function) Add the 2 rules created by AutomationSetup as a trigger for this Lambda. Replace REGION, ACCOUNT, ARNs and resource names with values from your account before running.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Smart Office Management System Workshop Overview Smart Office Management System provides a real-time environmental monitoring and management solution for offices, built entirely on AWS Serverless architecture to optimize costs and scalability.\nIn this lab, you will learn how to deploy, configure, and test a full-stack IoT system, allowing sensor devices to transmit data to the cloud and enabling administrators to control devices via a Web Dashboard.\nYou will work with two main architectural models to operate the Smart Office system:\nServerless Architecture - Uses AWS Lambda, API Gateway, and DynamoDB to handle logic and data storage. This model allows code to run in response to requests without managing servers. Event-Driven Architecture - Uses AWS IoT Core, EventBridge, and SNS. The system operates based on events, where data from sensors or user actions trigger automation workflows and send alert notifications. Content Workshop overview Prerequisite Run CloudFormation Stack Set up website Event Bridge SNS Test website IoT connection "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog: Monitoring &amp; Observability","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 6 Objectives: Implement Proactive Monitoring: Move from \u0026ldquo;checking if it works\u0026rdquo; to \u0026ldquo;knowing when it breaks\u0026rdquo; using Amazon CloudWatch. Centralize Logging: Configure EC2 instances to stream OS-level logs (syslog/application logs) to CloudWatch Logs. Automate Alerting: Set up an incident response pipeline using CloudWatch Alarms and Amazon SNS (Simple Notification Service). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Observability Fundamentals: + Metrics: Standard (CPU, Network) vs. Custom Metrics (RAM, Disk Usage). + Logs: Log groups, log streams, and retention policies. + Alarms: States (OK, ALARM, INSUFFICIENT_DATA). 09/08/2025 09/08/2025 What is Amazon CloudWatch? 3 - Practice: Install CloudWatch Agent: + Challenge: EC2 does not report RAM usage by default. + Attach IAM Role (CloudWatchAgentServerPolicy) to EC2. + Install and configure the Unified CloudWatch Agent to push Memory % and System Logs. 09/09/2025 09/09/2025 Collect metrics and logs with the CloudWatch Agent 4 - Practice: Create Operational Dashboards: + Create a custom CloudWatch Dashboard. + Add widgets for Key Performance Indicators (KPIs): CPU Utilization (Avg/Max), Network In/Out, and Estimated Charges. 09/10/2025 09/10/2025 Using Amazon CloudWatch Dashboards 5 - Practice: Alerting Pipeline (SNS): + Create an Amazon SNS Topic (e.g., DevOps-Alerts). + Subscribe your email address to the topic and confirm subscription. + Create a CloudWatch Alarm: Trigger if CPU Utilization \u0026gt; 70% for 2 periods of 5 minutes. + Link Alarm action to the SNS Topic. 09/11/2025 09/11/2025 Using Amazon CloudWatch Alarms 6 - Stress Test \u0026amp; Verification: + Run a stress tool (stress-ng) on the EC2 instance to spike CPU. + Verify the Alarm state changes to ALARM. + Confirm receipt of the alert email from AWS. 09/12/2025 09/12/2025 Self-Verification Week 6 Achievements: Established Full-Stack Observability:\nRecognized that default EC2 metrics (Hypervisor level) are insufficient for operations. Successfully deployed the CloudWatch Unified Agent to capture OS-level metrics (Memory/Disk usage) and logs. Implemented \u0026ldquo;Single Pane of Glass\u0026rdquo; Monitoring:\nCreated a Custom Dashboard providing a real-time view of the infrastructure\u0026rsquo;s health. Grouped metrics from the Load Balancer (ELB) and Compute (EC2) into a single view for faster troubleshooting. Automated Incident Response (ChatOps Prep):\nBuilt an automated alerting chain: Metric Spike → Alarm → SNS → Email. This setup allows the team to react to high-load events proactively before the system crashes, fulfilling the \u0026ldquo;Operational Excellence\u0026rdquo; pillar of the Well-Architected Framework. "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 06/08/2025 to 12/24/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Smart Office Project, through which I improved my skills in utilizing AWS Services, teamwork, CI/CD pipeline, web programming.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ☐ ✅ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Lack of self discipline Enhance communication skill for team work Be more \u0026ldquo;open\u0026rdquo; "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-sns/","title":"SNS Setup","tags":[],"description":"","content":"Overview This section will guide you through setting up Amazon SNS (Simple Notification Service) to receive alerts from EventBridge when data anomalies are detected.\nCreate SNS Topic Go to Amazon SNS Console Select Topic and name your Topic DON\u0026rsquo;T TURN ON ENCRYPTION\nCreate SNS Subscription After creating the topic, create one or more subscriptions so alerts are delivered to recipients or endpoints (in this case, email).\nSteps:\nSelect Subscription in SNS Choose email protocol, select the topic you just created, and write down the email you want to test. Email endpoints require confirmation before they receive messages. Check your inbox for a confirmation email from AWS SNS and confirm the subscription.\nCreate a rule and attach it to the SNS topic Go to Amazon EventBridge Console Select Rules and create a new rule In step 2, we define the event pattern by choosing Custom pattern. Event patterns can match on source, detail-type.\nUse the following Json:\n{ \u0026#34;source\u0026#34;: [\u0026#34;com.smartoffice.iot\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;sensor.anomaly\u0026#34;] } In step 3, Select the target for the rule — choose the SNS topic you created earlier. After creation, EventBridge will forward matching events to the SNS topic which will deliver to its subscriptions.\nReplace email addresses and resource names with values from your account before running.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nFCJ cultivates a welcoming and collaborative atmosphere where team members readily assist with challenges, even beyond regular hours. The workspace layout promotes concentration, though I believe periodic social events or team-building sessions could further enhance interpersonal connections among interns and staff.\n2. Support from Mentor / Team Admin\nMy mentor consistently delivers thorough instruction, breaks down complex concepts when I struggle, and actively fosters a question-friendly culture. The administrative team streamlines logistical needs and ensures access to essential resources. I particularly value the mentor\u0026rsquo;s coaching style—encouraging independent problem-solving before stepping in with solutions.\n3. Relevance of Work to Academic Major\nAssigned projects bridge classroom theory and emerging industry practices effectively. I reinforced core academic concepts while exploring technologies and methodologies not covered in my coursework, achieving a balance between consolidation and expansion of my technical repertoire.\n4. Learning \u0026amp; Skill Development Opportunities\nThe internship exposed me to project management platforms, cross-functional collaboration techniques, and professional communication norms. My mentor\u0026rsquo;s insights into career trajectory planning and real-world trade-offs proved invaluable for shaping my long-term goals.\n5. Company Culture \u0026amp; Team Spirit\nMutual respect and shared purpose define the organizational culture. Team members maintain high standards while preserving an approachable environment. During critical sprints, colleagues coordinate across levels to meet deadlines, making me feel integrated rather than peripheral despite my intern status.\n6. Internship Policies / Benefits\nFCJ offers a stipend and accommodates schedule adjustments when necessary. Access to internal training programs and knowledge-sharing sessions constitutes a significant perk that extends beyond typical internship offerings.\nAdditional Questions What did you find most satisfying during your internship? More connections with many fascinate people What do you think the company should improve for future interns? I honestly don\u0026rsquo;t know If recommending to a friend, would you suggest they intern here? Why or why not? Very good start if you ever want to be a Cloud Engineer, DevOps Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I honestly don\u0026rsquo;t know Would you like to continue this program in the future? I think yes Any other comments (free sharing): "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog: Content Delivery &amp; Edge Security","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 7 Objectives: Global Content Delivery: Reduce latency for end-users by caching static assets (images, CSS, JS) at Edge Locations using Amazon CloudFront. Secure the Origin: Implement Origin Access Control (OAC) to restrict S3 bucket access solely to CloudFront, disabling direct public access. Edge Security: Deploy AWS WAF (Web Application Firewall) to protect the application from common web exploits (e.g., SQLi, XSS) at the edge. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study CDN Fundamentals: + Edge vs. Region: Understanding the AWS Global Network. + Caching Behaviors: TTL (Time-to-Live), Cache Invalidation. + Security: SSL/TLS Termination and HTTPS enforcement. 09/15/2025 09/15/2025 What is Amazon CloudFront? 3 - Practice: CloudFront for S3 (Static Site): + Create a CloudFront Distribution pointing to the S3 bucket (from Week 4). + Critical Security Step: Configure Origin Access Control (OAC). + Update S3 Bucket Policy to deny direct internet access and allow only CloudFront. 09/16/2025 09/16/2025 Restricting access to an S3 origin 4 - Practice: Cache Optimization: + Observe headers in the browser DevTools (X-Cache: Miss vs. Hit). + Configure Cache Behaviors to compress objects automatically (Gzip/Brotli). + Invalidation: Practice manually invalidating the cache when updating index.html. 09/17/2025 09/17/2025 Managing Cache Expiration 5 - Practice: Implement AWS WAF: + Create a Web ACL (Access Control List). + Add a Managed Rule (e.g., AWSManagedRulesCommonRuleSet to block SQL Injection). + Associate the Web ACL with the CloudFront Distribution. 09/18/2025 09/18/2025 AWS WAF Web ACLs 6 - Verification \u0026amp; Benchmarking: + Compare load times: Direct S3 URL (High Latency) vs. CloudFront URL (Low Latency). + Test WAF: Attempt a simulated attack (e.g., passing a script tag in URL) and verify a 403 Forbidden response. 09/19/2025 09/19/2025 Self-Verification Week 7 Achievements: Optimized Performance (Latency Reduction):\nDeployed Amazon CloudFront to serve static assets from Edge Locations closest to the user. Verified a significant reduction in load times and achieved a high Cache Hit Ratio, reducing the load on the Origin server. Secured the Origin (S3 Hardening):\nImplemented Origin Access Control (OAC), the modern standard for S3-CloudFront security. Successfully revoked public read access on the S3 bucket, ensuring that users can only access content through the secure CDN endpoint (DevSecOps best practice). Implemented Perimeter Security (WAF):\nIntegrated AWS WAF with the CloudFront distribution. Applied managed rule sets to filter malicious traffic (OWASP Top 10 threats) before it even reaches the application infrastructure. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-test-website-iot-connection/","title":"Test website and IoT connection","tags":[],"description":"","content":"Download mock IoT device script main.py\nAdd data from your mock device ENDPOINT = \u0026#34;\u0026#34; CLIENT_ID = \u0026#34;\u0026#34; OFFICE_ID = \u0026#34;\u0026#34; ROOM_ID = \u0026#34;\u0026#34; PATH_TO_CERT = \u0026#34;\u0026#34; PATH_TO_KEY = \u0026#34;\u0026#34; PATH_TO_ROOT = \u0026#34;\u0026#34; Attribute Value ENDPOINT Website manager page CLIENT_ID ID of your office OFFICE_ID Name of your office ROOM_ID ID of your room PATH_TO_CERT Path to your device certificate download when your create room PATH_TO_KEY Path to your device private key download when your create room PATH_TO_ROOT Path to your Amazon root certifica\tdownload when your create room Link video demo https://www.youtube.com/watch?v=k45jHjkKhuc\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review knowledge for mid-term test Taking mid-term test Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review mid-term test content 10/27/2025 10/27/2025 3 - Review mid-term test content 10/28/2025 10/28/2025 4 - Review mid-term test content 10/29/2025 10/29/2025 5 - Doing quiz created by NotebookLM for mid-term test 10/30/2025 10/30/2025 6 - Take mid-term test 10/31/2025 10/31/2025 Week 8 Achievements: Finish mid-term test "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog: Project Kickoff - Smart Office IoT Core","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report.\nWeek 9 Objectives: Project Initialization: Understand the \u0026ldquo;Smart Office\u0026rdquo; architecture and requirements (Sensors: Temperature, Light, Door Status). IoT Infrastructure Setup: Configure AWS IoT Core to act as the central message broker. Device Connectivity: Register \u0026ldquo;Things\u0026rdquo; and establish secure MQTT communication using X.509 certificates. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Project Kickoff \u0026amp; Architecture Design: + Analyze the Smart Office requirements. + Design the flow: Device -\u0026gt; IoT Core -\u0026gt; DynamoDB/SNS. + Define the MQTT Topics structure (e.g., smart-office/room1/temp). 11/03/2025 11/03/2025 AWS IoT Core Architecture 3 - Configure AWS IoT Core: + Create an IoT Policy allowing iot:Connect, iot:Publish, and iot:Subscribe. + Create a Thing in the registry (e.g., Virtual_Temp_Sensor). 11/04/2025 11/04/2025 Create AWS IoT Things 4 - Device Security (Certificates): + Generate X.509 Certificates (Public Key, Private Key, Root CA). + Attach the IoT Policy and Certificate to the Thing. + DevSecOps: Ensure private keys are stored securely. 11/05/2025 11/05/2025 X.509 Client Certificates 5 - Simulate IoT Device: + Write a script (Python/Node.js) using AWS IoT Device SDK. + Simulate sensor data (e.g., sending random temperature values). + Connect to the IoT Core Endpoint. 11/06/2025 11/06/2025 AWS IoT Device SDKs 6 - Verify MQTT Messaging: + Use the MQTT Test Client in AWS Console. + Subscribe to # (wildcard) or specific topic. + Verify that data sent from the script appears in the Console in real-time. 11/07/2025 11/07/2025 View MQTT messages Week 9 Achievements: Architected the Smart Office Solution:\nDefined the data flow and topic hierarchy for the Smart Office environment. Selected MQTT as the lightweight communication protocol suitable for sensor networks. Provisioned IoT Infrastructure:\nSuccessfully set up AWS IoT Core resources. Created and registered \u0026ldquo;Things\u0026rdquo; representing the physical/virtual sensors of the office. Established Secure Device Communication:\nImplemented Mutual TLS (mTLS) authentication using X.509 certificates. Successfully simulated a sensor device sending telemetry data to the cloud, verified via the MQTT Test Client. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Prepare for university final exam in Academic Writing Complete Smart Office IoT project implementation and documentation Review and practice academic writing skills for exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review academic writing fundamentals - Practice essay structure and organization - Study thesis statement development 11/25/2025 11/25/2025 Academic Writing course materials 3 - Practice writing argumentative essays - Review citation and referencing styles - Study paragraph development techniques 11/26/2025 11/26/2025 Academic Writing textbooks 4 - Finalize Smart Office IoT project code - Complete Lambda functions for Rules Engine - Test DynamoDB data persistence - Verify all IoT Core integrations 11/27/2025 11/28/2025 Project documentation 5 - Build serverless API with API Gateway - Create CloudWatch dashboards for visualization - Test end-to-end project workflow - Document architecture and implementation 11/28/2025 11/29/2025 AWS Documentation 6 - Practice timed essay writing - Review common essay topics and prompts - Prepare for final exam format - Complete final review and mock exam 11/29/2025 11/29/2025 University exam guidelines Week 12 Achievements: Prepared thoroughly for Academic Writing final exam\nReviewed essay structure: introduction, body paragraphs, and conclusion Practiced thesis statement formulation and argument development Studied various essay types: argumentative, expository, and analytical Mastered citation formats (APA, MLA) and academic tone Completed multiple practice essays under timed conditions Finalized Smart Office IoT Core project implementation:\nCompleted Lambda functions for processing IoT sensor data Configured EventBridge rules for automated workflows Set up DynamoDB tables for persistent data storage Integrated SNS for alert notifications Built API Gateway endpoints for data retrieval Created CloudWatch dashboards for real-time monitoring Prepared comprehensive project documentation:\nArchitecture diagrams showing all AWS services and their interactions Step-by-step deployment guide with CloudFormation templates API documentation with sample requests and responses Test results demonstrating system functionality Delivered final project presentation:\nDemonstrated complete end-to-end workflow Explained technical decisions and AWS service choices Highlighted challenges faced and solutions implemented Showcased visualization dashboard and alerting system Reflected on internship experience:\nGained hands-on experience with 15+ AWS services Developed skills in serverless architecture and IoT solutions Improved understanding of cloud best practices and Well-Architected Framework Built portfolio project demonstrating real-world AWS implementation "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.13-week13/","title":"Week 13 Worklog: Coursera AWS Certification","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 13 Objectives: Complete academic writing course on Coursera to improve technical documentation skills Enhance ability to write clear, structured, and professional reports Apply writing techniques to internship deliverables and technical content Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Enroll in Coursera academic writing course - Study writing fundamentals module - Learn about structure, clarity, and audience awareness 12/02/2025 12/02/2025 Coursera Academic Writing Course 3 - Complete grammar and style module - Practice sentence construction and paragraph development - Review common writing errors and how to avoid them 12/03/2025 12/03/2025 Coursera Academic Writing Course 4 - Study technical writing principles - Learn documentation best practices - Practice writing clear technical descriptions 12/04/2025 12/04/2025 Coursera Academic Writing Course 5 - Study research and citation methods - Learn how to structure arguments and present evidence - Practice writing persuasive and informative content 12/05/2025 12/05/2025 Coursera Academic Writing Course 6 - Complete final writing assignments - Apply learned techniques to internship report - Review and revise existing documentation 12/06/2025 12/06/2025 Coursera Academic Writing Course Week 13 Achievements: Academic Writing Enhancement:\nMastered fundamental principles of clear and effective academic writing Developed stronger grammar, punctuation, and sentence structure skills Learned to organize ideas logically with proper transitions and flow Improved ability to write for specific audiences and purposes Enhanced technical documentation skills applicable to cloud engineering Applied writing techniques to internship deliverables:\nRevised worklog entries with improved clarity and professionalism Enhanced technical descriptions in project documentation Improved structure and coherence of workshop translations Refined blog summaries with better synthesis of complex information Strengthened event participation narratives with clear takeaways Developed professional writing habits:\nPlanning content structure before writing Using active voice and concise language Proofreading and editing systematically Seeking feedback and iterating on drafts Maintaining consistent tone and style throughout documents Gained confidence in written communication:\nAbility to explain technical concepts clearly to diverse audiences Skills to produce polished, error-free professional documents Understanding of how to structure reports, proposals, and analyses Competence in citing sources and acknowledging references appropriately "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]